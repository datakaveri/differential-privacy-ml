{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTRpH6YcQXvJ",
        "outputId": "5172e2d5-107c-4051-8298-eb761ce13f2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opacus in /usr/local/lib/python3.8/dist-packages (1.3.0)\n",
            "Requirement already satisfied: functorch in /usr/local/lib/python3.8/dist-packages (from opacus) (1.13.1)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.8/dist-packages (from opacus) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from opacus) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.8/dist-packages (from opacus) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.8/dist-packages (from opacus) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8->opacus) (4.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install opacus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c2-4YyQ1QYh7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vpEhmCODQYk6"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "\n",
        "pca = PCA(n_components=60)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "minmax = MinMaxScaler()\n",
        "X_train_pca = minmax.fit_transform(X_train_pca)\n",
        "X_test_pca = minmax.transform(X_test_pca)\n",
        "\n",
        "if torch.cuda.is_available(): \n",
        "    dev = \"cuda:0\" \n",
        "else: \n",
        "    dev = \"cpu\" \n",
        "device = torch.device(dev)\n",
        "\n",
        "class MNISTDatasetTrain(Dataset):\n",
        "    def __init__(self):\n",
        "        self.X_train_pca = X_train_pca\n",
        "        self.y_train = y_train\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X_train_pca)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.Tensor(self.X_train_pca[idx]), torch.tensor([self.y_train[idx]])\n",
        "    \n",
        "class MNISTDatasetTest(Dataset):\n",
        "    def __init__(self):\n",
        "        self.X_test_pca = X_test_pca\n",
        "        self.y_test = y_test\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X_test_pca)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.Tensor(self.X_test_pca[idx]), torch.tensor([self.y_test[idx]])\n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58fOPLXjSMRI",
        "outputId": "ef9af331-9acb-4e9f-f673-2a3fc6086935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wBCWvIfXQYno",
        "outputId": "31cc77dc-d6af-4ae5-e9cf-88899a82e20e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERRORS:  []\n",
            "Using sigma=0.8023 and C=4\n",
            "Training Epoch: 1 [--------------------]\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch: 1 [====================] (60157/60000)\n",
            "Trained Epoch: 1 with loss= 1.912545, accuracy= 55.28, and epsilon= 1.51\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/opacus/accountants/analysis/prv/prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
            "  z = np.log((np.exp(t) + q - 1) / q)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set metrics: \tAvg. loss: 1.3726, \tAccuracy: 7911/10000 (79.11%)\n",
            "\n",
            "Training Epoch: 2 [====================] (59893/60000)\n",
            "Trained Epoch: 2 with loss= 0.980454, accuracy= 81.05, and epsilon= 1.80\n",
            "Test set metrics: \tAvg. loss: 0.6728, \tAccuracy: 8544/10000 (85.44%)\n",
            "\n",
            "Training Epoch: 3 [====================] (60612/60000)\n",
            "Trained Epoch: 3 with loss= 0.585684, accuracy= 86.38, and epsilon= 2.02\n",
            "Test set metrics: \tAvg. loss: 0.4893, \tAccuracy: 8746/10000 (87.46%)\n",
            "\n",
            "Training Epoch: 4 [====================] (59872/60000)\n",
            "Trained Epoch: 4 with loss= 0.472335, accuracy= 86.49, and epsilon= 2.21\n",
            "Test set metrics: \tAvg. loss: 0.4196, \tAccuracy: 8839/10000 (88.39%)\n",
            "\n",
            "Training Epoch: 5 [====================] (60188/60000)\n",
            "Trained Epoch: 5 with loss= 0.426240, accuracy= 87.88, and epsilon= 2.39\n",
            "Test set metrics: \tAvg. loss: 0.3848, \tAccuracy: 8896/10000 (88.96%)\n",
            "\n",
            "Training Epoch: 6 [====================] (60052/60000)\n",
            "Trained Epoch: 6 with loss= 0.406196, accuracy= 88.14, and epsilon= 2.55\n",
            "Test set metrics: \tAvg. loss: 0.3675, \tAccuracy: 8945/10000 (89.45%)\n",
            "\n",
            "Training Epoch: 7 [====================] (60080/60000)\n",
            "Trained Epoch: 7 with loss= 0.397234, accuracy= 88.41, and epsilon= 2.71\n",
            "Test set metrics: \tAvg. loss: 0.3589, \tAccuracy: 8960/10000 (89.60%)\n",
            "\n",
            "Training Epoch: 8 [====================] (59790/60000)\n",
            "Trained Epoch: 8 with loss= 0.377145, accuracy= 88.41, and epsilon= 2.85\n",
            "Test set metrics: \tAvg. loss: 0.3472, \tAccuracy: 8989/10000 (89.89%)\n",
            "\n",
            "Training Epoch: 9 [====================] (59912/60000)\n",
            "Trained Epoch: 9 with loss= 0.378800, accuracy= 88.79, and epsilon= 2.99\n",
            "Test set metrics: \tAvg. loss: 0.3474, \tAccuracy: 9008/10000 (90.08%)\n",
            "\n",
            "Training Epoch: 10 [====================] (59699/60000)\n",
            "Trained Epoch: 10 with loss= 0.372904, accuracy= 88.64, and epsilon= 3.13\n",
            "Test set metrics: \tAvg. loss: 0.3390, \tAccuracy: 9029/10000 (90.29%)\n",
            "\n",
            "Training Epoch: 11 [====================] (59735/60000)\n",
            "Trained Epoch: 11 with loss= 0.362334, accuracy= 89.08, and epsilon= 3.26\n",
            "Test set metrics: \tAvg. loss: 0.3417, \tAccuracy: 9041/10000 (90.41%)\n",
            "\n",
            "Training Epoch: 12 [====================] (59978/60000)\n",
            "Trained Epoch: 12 with loss= 0.364895, accuracy= 89.50, and epsilon= 3.38\n",
            "Test set metrics: \tAvg. loss: 0.3384, \tAccuracy: 9025/10000 (90.25%)\n",
            "\n",
            "Training Epoch: 13 [====================] (60064/60000)\n",
            "Trained Epoch: 13 with loss= 0.359643, accuracy= 89.87, and epsilon= 3.50\n",
            "Test set metrics: \tAvg. loss: 0.3380, \tAccuracy: 9052/10000 (90.52%)\n",
            "\n",
            "Training Epoch: 14 [====================] (59891/60000)\n",
            "Trained Epoch: 14 with loss= 0.360282, accuracy= 89.69, and epsilon= 3.62\n",
            "Test set metrics: \tAvg. loss: 0.3375, \tAccuracy: 9078/10000 (90.78%)\n",
            "\n",
            "Training Epoch: 15 [====================] (60015/60000)\n",
            "Trained Epoch: 15 with loss= 0.374848, accuracy= 89.48, and epsilon= 3.74\n",
            "Test set metrics: \tAvg. loss: 0.3339, \tAccuracy: 9065/10000 (90.65%)\n",
            "\n",
            "Training Epoch: 16 [====================] (59826/60000)\n",
            "Trained Epoch: 16 with loss= 0.360363, accuracy= 89.56, and epsilon= 3.85\n",
            "Test set metrics: \tAvg. loss: 0.3366, \tAccuracy: 9088/10000 (90.88%)\n",
            "\n",
            "Training Epoch: 17 [====================] (60423/60000)\n",
            "Trained Epoch: 17 with loss= 0.367604, accuracy= 90.48, and epsilon= 3.96\n",
            "Test set metrics: \tAvg. loss: 0.3348, \tAccuracy: 9095/10000 (90.95%)\n",
            "\n",
            "Training Epoch: 18 [====================] (60001/60000)\n",
            "Trained Epoch: 18 with loss= 0.360688, accuracy= 90.03, and epsilon= 4.06\n",
            "Test set metrics: \tAvg. loss: 0.3334, \tAccuracy: 9106/10000 (91.06%)\n",
            "\n",
            "Training Epoch: 19 [====================] (59707/60000)\n",
            "Trained Epoch: 19 with loss= 0.365791, accuracy= 89.80, and epsilon= 4.17\n",
            "Test set metrics: \tAvg. loss: 0.3283, \tAccuracy: 9111/10000 (91.11%)\n",
            "\n",
            "Training Epoch: 20 [====================] (60315/60000)\n",
            "Trained Epoch: 20 with loss= 0.361080, accuracy= 90.83, and epsilon= 4.27\n",
            "Test set metrics: \tAvg. loss: 0.3327, \tAccuracy: 9110/10000 (91.10%)\n",
            "\n",
            "Training Epoch: 21 [====================] (60183/60000)\n",
            "Trained Epoch: 21 with loss= 0.367208, accuracy= 90.37, and epsilon= 4.37\n",
            "Test set metrics: \tAvg. loss: 0.3305, \tAccuracy: 9110/10000 (91.10%)\n",
            "\n",
            "Training Epoch: 22 [====================] (59946/60000)\n",
            "Trained Epoch: 22 with loss= 0.367129, accuracy= 90.07, and epsilon= 4.47\n",
            "Test set metrics: \tAvg. loss: 0.3307, \tAccuracy: 9122/10000 (91.22%)\n",
            "\n",
            "Training Epoch: 23 [====================] (59967/60000)\n",
            "Trained Epoch: 23 with loss= 0.353029, accuracy= 90.42, and epsilon= 4.57\n",
            "Test set metrics: \tAvg. loss: 0.3328, \tAccuracy: 9123/10000 (91.23%)\n",
            "\n",
            "Training Epoch: 24 [====================] (60109/60000)\n",
            "Trained Epoch: 24 with loss= 0.358945, accuracy= 90.51, and epsilon= 4.67\n",
            "Test set metrics: \tAvg. loss: 0.3324, \tAccuracy: 9127/10000 (91.27%)\n",
            "\n",
            "Training Epoch: 25 [====================] (59834/60000)\n",
            "Trained Epoch: 25 with loss= 0.362543, accuracy= 90.31, and epsilon= 4.76\n",
            "Test set metrics: \tAvg. loss: 0.3339, \tAccuracy: 9124/10000 (91.24%)\n",
            "\n",
            "Training Epoch: 26 [====================] (59625/60000)\n",
            "Trained Epoch: 26 with loss= 0.355629, accuracy= 90.09, and epsilon= 4.85\n",
            "Test set metrics: \tAvg. loss: 0.3307, \tAccuracy: 9129/10000 (91.29%)\n",
            "\n",
            "Training Epoch: 27 [====================] (59844/60000)\n",
            "Trained Epoch: 27 with loss= 0.356393, accuracy= 90.41, and epsilon= 4.95\n",
            "Test set metrics: \tAvg. loss: 0.3379, \tAccuracy: 9109/10000 (91.09%)\n",
            "\n",
            "Training Epoch: 28 [====================] (59884/60000)\n",
            "Trained Epoch: 28 with loss= 0.358964, accuracy= 90.36, and epsilon= 5.04\n",
            "Test set metrics: \tAvg. loss: 0.3396, \tAccuracy: 9116/10000 (91.16%)\n",
            "\n",
            "Training Epoch: 29 [====================] (60140/60000)\n",
            "Trained Epoch: 29 with loss= 0.364957, accuracy= 90.57, and epsilon= 5.13\n",
            "Test set metrics: \tAvg. loss: 0.3411, \tAccuracy: 9118/10000 (91.18%)\n",
            "\n",
            "Training Epoch: 30 [====================] (59545/60000)\n",
            "Trained Epoch: 30 with loss= 0.369400, accuracy= 89.82, and epsilon= 5.21\n",
            "Test set metrics: \tAvg. loss: 0.3360, \tAccuracy: 9134/10000 (91.34%)\n",
            "\n",
            "Training Epoch: 31 [====================] (60347/60000)\n",
            "Trained Epoch: 31 with loss= 0.353132, accuracy= 91.19, and epsilon= 5.30\n",
            "Test set metrics: \tAvg. loss: 0.3414, \tAccuracy: 9131/10000 (91.31%)\n",
            "\n",
            "Training Epoch: 32 [====================] (59923/60000)\n",
            "Trained Epoch: 32 with loss= 0.360630, accuracy= 90.56, and epsilon= 5.39\n",
            "Test set metrics: \tAvg. loss: 0.3406, \tAccuracy: 9135/10000 (91.35%)\n",
            "\n",
            "Training Epoch: 33 [====================] (59553/60000)\n",
            "Trained Epoch: 33 with loss= 0.360075, accuracy= 90.17, and epsilon= 5.47\n",
            "Test set metrics: \tAvg. loss: 0.3428, \tAccuracy: 9122/10000 (91.22%)\n",
            "\n",
            "Training Epoch: 34 [====================] (60542/60000)\n",
            "Trained Epoch: 34 with loss= 0.369763, accuracy= 91.46, and epsilon= 5.56\n",
            "Test set metrics: \tAvg. loss: 0.3385, \tAccuracy: 9132/10000 (91.32%)\n",
            "\n",
            "Training Epoch: 35 [====================] (60276/60000)\n",
            "Trained Epoch: 35 with loss= 0.370099, accuracy= 90.95, and epsilon= 5.64\n",
            "Test set metrics: \tAvg. loss: 0.3392, \tAccuracy: 9146/10000 (91.46%)\n",
            "\n",
            "Training Epoch: 36 [====================] (59835/60000)\n",
            "Trained Epoch: 36 with loss= 0.371903, accuracy= 90.37, and epsilon= 5.72\n",
            "Test set metrics: \tAvg. loss: 0.3355, \tAccuracy: 9149/10000 (91.49%)\n",
            "\n",
            "Training Epoch: 37 [====================] (59827/60000)\n",
            "Trained Epoch: 37 with loss= 0.360407, accuracy= 90.51, and epsilon= 5.81\n",
            "Test set metrics: \tAvg. loss: 0.3441, \tAccuracy: 9143/10000 (91.43%)\n",
            "\n",
            "Training Epoch: 38 [====================] (60309/60000)\n",
            "Trained Epoch: 38 with loss= 0.366924, accuracy= 91.31, and epsilon= 5.89\n",
            "Test set metrics: \tAvg. loss: 0.3379, \tAccuracy: 9156/10000 (91.56%)\n",
            "\n",
            "Training Epoch: 39 [====================] (60017/60000)\n",
            "Trained Epoch: 39 with loss= 0.356393, accuracy= 90.85, and epsilon= 5.97\n",
            "Test set metrics: \tAvg. loss: 0.3409, \tAccuracy: 9126/10000 (91.26%)\n",
            "\n",
            "Training Epoch: 40 [====================] (59543/60000)\n",
            "Trained Epoch: 40 with loss= 0.374295, accuracy= 90.11, and epsilon= 6.05\n",
            "Test set metrics: \tAvg. loss: 0.3396, \tAccuracy: 9148/10000 (91.48%)\n",
            "\n",
            "Training Epoch: 41 [====================] (60096/60000)\n",
            "Trained Epoch: 41 with loss= 0.361677, accuracy= 91.13, and epsilon= 6.12\n",
            "Test set metrics: \tAvg. loss: 0.3390, \tAccuracy: 9166/10000 (91.66%)\n",
            "\n",
            "Training Epoch: 42 [====================] (59885/60000)\n",
            "Trained Epoch: 42 with loss= 0.365836, accuracy= 90.66, and epsilon= 6.20\n",
            "Test set metrics: \tAvg. loss: 0.3382, \tAccuracy: 9160/10000 (91.60%)\n",
            "\n",
            "Training Epoch: 43 [====================] (59811/60000)\n",
            "Trained Epoch: 43 with loss= 0.370900, accuracy= 90.55, and epsilon= 6.28\n",
            "Test set metrics: \tAvg. loss: 0.3412, \tAccuracy: 9152/10000 (91.52%)\n",
            "\n",
            "Training Epoch: 44 [====================] (60096/60000)\n",
            "Trained Epoch: 44 with loss= 0.366041, accuracy= 91.00, and epsilon= 6.36\n",
            "Test set metrics: \tAvg. loss: 0.3428, \tAccuracy: 9152/10000 (91.52%)\n",
            "\n",
            "Training Epoch: 45 [====================] (59543/60000)\n",
            "Trained Epoch: 45 with loss= 0.360200, accuracy= 90.27, and epsilon= 6.43\n",
            "Test set metrics: \tAvg. loss: 0.3405, \tAccuracy: 9151/10000 (91.51%)\n",
            "\n",
            "Training Epoch: 46 [====================] (60181/60000)\n",
            "Trained Epoch: 46 with loss= 0.362372, accuracy= 91.47, and epsilon= 6.51\n",
            "Test set metrics: \tAvg. loss: 0.3461, \tAccuracy: 9128/10000 (91.28%)\n",
            "\n",
            "Training Epoch: 47 [====================] (60273/60000)\n",
            "Trained Epoch: 47 with loss= 0.364181, accuracy= 91.43, and epsilon= 6.58\n",
            "Test set metrics: \tAvg. loss: 0.3442, \tAccuracy: 9164/10000 (91.64%)\n",
            "\n",
            "Training Epoch: 48 [====================] (59961/60000)\n",
            "Trained Epoch: 48 with loss= 0.356980, accuracy= 91.16, and epsilon= 6.66\n",
            "Test set metrics: \tAvg. loss: 0.3487, \tAccuracy: 9155/10000 (91.55%)\n",
            "\n",
            "Training Epoch: 49 [====================] (59793/60000)\n",
            "Trained Epoch: 49 with loss= 0.358943, accuracy= 90.77, and epsilon= 6.73\n",
            "Test set metrics: \tAvg. loss: 0.3472, \tAccuracy: 9172/10000 (91.72%)\n",
            "\n",
            "Training Epoch: 50 [====================] (59512/60000)\n",
            "Trained Epoch: 50 with loss= 0.366823, accuracy= 90.43, and epsilon= 6.80\n",
            "Test set metrics: \tAvg. loss: 0.3559, \tAccuracy: 9152/10000 (91.52%)\n",
            "\n",
            "Training Epoch: 51 [====================] (59936/60000)\n",
            "Trained Epoch: 51 with loss= 0.361439, accuracy= 90.99, and epsilon= 6.88\n",
            "Test set metrics: \tAvg. loss: 0.3518, \tAccuracy: 9173/10000 (91.73%)\n",
            "\n",
            "Training Epoch: 52 [====================] (60118/60000)\n",
            "Trained Epoch: 52 with loss= 0.360656, accuracy= 91.40, and epsilon= 6.95\n",
            "Test set metrics: \tAvg. loss: 0.3576, \tAccuracy: 9146/10000 (91.46%)\n",
            "\n",
            "Training Epoch: 53 [====================] (59836/60000)\n",
            "Trained Epoch: 53 with loss= 0.376606, accuracy= 90.72, and epsilon= 7.02\n",
            "Test set metrics: \tAvg. loss: 0.3542, \tAccuracy: 9155/10000 (91.55%)\n",
            "\n",
            "Training Epoch: 54 [====================] (59556/60000)\n",
            "Trained Epoch: 54 with loss= 0.369751, accuracy= 90.38, and epsilon= 7.09\n",
            "Test set metrics: \tAvg. loss: 0.3528, \tAccuracy: 9143/10000 (91.43%)\n",
            "\n",
            "Training Epoch: 55 [====================] (60252/60000)\n",
            "Trained Epoch: 55 with loss= 0.367594, accuracy= 91.50, and epsilon= 7.16\n",
            "Test set metrics: \tAvg. loss: 0.3470, \tAccuracy: 9147/10000 (91.47%)\n",
            "\n",
            "Training Epoch: 56 [====================] (60087/60000)\n",
            "Trained Epoch: 56 with loss= 0.368920, accuracy= 91.27, and epsilon= 7.23\n",
            "Test set metrics: \tAvg. loss: 0.3443, \tAccuracy: 9186/10000 (91.86%)\n",
            "\n",
            "Training Epoch: 57 [====================] (60636/60000)\n",
            "Trained Epoch: 57 with loss= 0.374268, accuracy= 92.17, and epsilon= 7.30\n",
            "Test set metrics: \tAvg. loss: 0.3477, \tAccuracy: 9167/10000 (91.67%)\n",
            "\n",
            "Training Epoch: 58 [====================] (59928/60000)\n",
            "Trained Epoch: 58 with loss= 0.362451, accuracy= 91.21, and epsilon= 7.37\n",
            "Test set metrics: \tAvg. loss: 0.3475, \tAccuracy: 9186/10000 (91.86%)\n",
            "\n",
            "Training Epoch: 59 [====================] (60172/60000)\n",
            "Trained Epoch: 59 with loss= 0.366539, accuracy= 91.44, and epsilon= 7.44\n",
            "Test set metrics: \tAvg. loss: 0.3433, \tAccuracy: 9202/10000 (92.02%)\n",
            "\n",
            "Training Epoch: 60 [====================] (60175/60000)\n",
            "Trained Epoch: 60 with loss= 0.367355, accuracy= 91.62, and epsilon= 7.51\n",
            "Test set metrics: \tAvg. loss: 0.3465, \tAccuracy: 9194/10000 (91.94%)\n",
            "\n",
            "Training Epoch: 61 [====================] (59801/60000)\n",
            "Trained Epoch: 61 with loss= 0.361495, accuracy= 91.15, and epsilon= 7.58\n",
            "Test set metrics: \tAvg. loss: 0.3461, \tAccuracy: 9178/10000 (91.78%)\n",
            "\n",
            "Training Epoch: 62 [====================] (59785/60000)\n",
            "Trained Epoch: 62 with loss= 0.364375, accuracy= 91.14, and epsilon= 7.64\n",
            "Test set metrics: \tAvg. loss: 0.3499, \tAccuracy: 9169/10000 (91.69%)\n",
            "\n",
            "Training Epoch: 63 [====================] (59856/60000)\n",
            "Trained Epoch: 63 with loss= 0.370638, accuracy= 91.17, and epsilon= 7.71\n",
            "Test set metrics: \tAvg. loss: 0.3475, \tAccuracy: 9199/10000 (91.99%)\n",
            "\n",
            "Training Epoch: 64 [====================] (59732/60000)\n",
            "Trained Epoch: 64 with loss= 0.373518, accuracy= 90.92, and epsilon= 7.78\n",
            "Test set metrics: \tAvg. loss: 0.3449, \tAccuracy: 9195/10000 (91.95%)\n",
            "\n",
            "Training Epoch: 65 [====================] (59920/60000)\n",
            "Trained Epoch: 65 with loss= 0.365278, accuracy= 91.26, and epsilon= 7.85\n",
            "Test set metrics: \tAvg. loss: 0.3432, \tAccuracy: 9194/10000 (91.94%)\n",
            "\n",
            "Training Epoch: 66 [====================] (60006/60000)\n",
            "Trained Epoch: 66 with loss= 0.370141, accuracy= 91.39, and epsilon= 7.91\n",
            "Test set metrics: \tAvg. loss: 0.3500, \tAccuracy: 9183/10000 (91.83%)\n",
            "\n",
            "Training Epoch: 67 [====================] (60212/60000)\n",
            "Trained Epoch: 67 with loss= 0.375510, accuracy= 91.47, and epsilon= 7.98\n",
            "Test set metrics: \tAvg. loss: 0.3578, \tAccuracy: 9190/10000 (91.90%)\n",
            "\n",
            "Training Epoch: 68 [====================] (59853/60000)\n",
            "Trained Epoch: 68 with loss= 0.375376, accuracy= 91.09, and epsilon= 8.04\n",
            "Test set metrics: \tAvg. loss: 0.3552, \tAccuracy: 9183/10000 (91.83%)\n",
            "\n",
            "Training Epoch: 69 [====================] (59986/60000)\n",
            "Trained Epoch: 69 with loss= 0.364161, accuracy= 91.46, and epsilon= 8.11\n",
            "Test set metrics: \tAvg. loss: 0.3535, \tAccuracy: 9206/10000 (92.06%)\n",
            "\n",
            "Training Epoch: 70 [====================] (60056/60000)\n",
            "Trained Epoch: 70 with loss= 0.376249, accuracy= 91.58, and epsilon= 8.17\n",
            "Test set metrics: \tAvg. loss: 0.3493, \tAccuracy: 9211/10000 (92.11%)\n",
            "\n",
            "Training Epoch: 71 [====================] (60066/60000)\n",
            "Trained Epoch: 71 with loss= 0.372008, accuracy= 91.68, and epsilon= 8.24\n",
            "Test set metrics: \tAvg. loss: 0.3538, \tAccuracy: 9186/10000 (91.86%)\n",
            "\n",
            "Training Epoch: 72 [====================] (59738/60000)\n",
            "Trained Epoch: 72 with loss= 0.359336, accuracy= 91.24, and epsilon= 8.30\n",
            "Test set metrics: \tAvg. loss: 0.3494, \tAccuracy: 9198/10000 (91.98%)\n",
            "\n",
            "Training Epoch: 73 [====================] (59673/60000)\n",
            "Trained Epoch: 73 with loss= 0.378808, accuracy= 90.97, and epsilon= 8.37\n",
            "Test set metrics: \tAvg. loss: 0.3452, \tAccuracy: 9199/10000 (91.99%)\n",
            "\n",
            "Training Epoch: 74 [====================] (60094/60000)\n",
            "Trained Epoch: 74 with loss= 0.367059, accuracy= 91.66, and epsilon= 8.43\n",
            "Test set metrics: \tAvg. loss: 0.3436, \tAccuracy: 9193/10000 (91.93%)\n",
            "\n",
            "Training Epoch: 75 [====================] (59994/60000)\n",
            "Trained Epoch: 75 with loss= 0.383865, accuracy= 91.26, and epsilon= 8.49\n",
            "Test set metrics: \tAvg. loss: 0.3509, \tAccuracy: 9190/10000 (91.90%)\n",
            "\n",
            "Training Epoch: 76 [====================] (59799/60000)\n",
            "Trained Epoch: 76 with loss= 0.357220, accuracy= 91.25, and epsilon= 8.56\n",
            "Test set metrics: \tAvg. loss: 0.3585, \tAccuracy: 9177/10000 (91.77%)\n",
            "\n",
            "Training Epoch: 77 [====================] (59865/60000)\n",
            "Trained Epoch: 77 with loss= 0.377265, accuracy= 91.33, and epsilon= 8.62\n",
            "Test set metrics: \tAvg. loss: 0.3531, \tAccuracy: 9197/10000 (91.97%)\n",
            "\n",
            "Training Epoch: 78 [====================] (59711/60000)\n",
            "Trained Epoch: 78 with loss= 0.370842, accuracy= 91.21, and epsilon= 8.68\n",
            "Test set metrics: \tAvg. loss: 0.3501, \tAccuracy: 9203/10000 (92.03%)\n",
            "\n",
            "Training Epoch: 79 [====================] (59866/60000)\n",
            "Trained Epoch: 79 with loss= 0.371931, accuracy= 91.39, and epsilon= 8.74\n",
            "Test set metrics: \tAvg. loss: 0.3596, \tAccuracy: 9204/10000 (92.04%)\n",
            "\n",
            "Training Epoch: 80 [====================] (60103/60000)\n",
            "Trained Epoch: 80 with loss= 0.367927, accuracy= 91.86, and epsilon= 8.81\n",
            "Test set metrics: \tAvg. loss: 0.3488, \tAccuracy: 9210/10000 (92.10%)\n",
            "\n",
            "Training Epoch: 81 [====================] (60389/60000)\n",
            "Trained Epoch: 81 with loss= 0.374342, accuracy= 92.13, and epsilon= 8.87\n",
            "Test set metrics: \tAvg. loss: 0.3510, \tAccuracy: 9200/10000 (92.00%)\n",
            "\n",
            "Training Epoch: 82 [====================] (60053/60000)\n",
            "Trained Epoch: 82 with loss= 0.360832, accuracy= 91.88, and epsilon= 8.93\n",
            "Test set metrics: \tAvg. loss: 0.3512, \tAccuracy: 9224/10000 (92.24%)\n",
            "\n",
            "Training Epoch: 83 [====================] (59860/60000)\n",
            "Trained Epoch: 83 with loss= 0.366017, accuracy= 91.34, and epsilon= 8.99\n",
            "Test set metrics: \tAvg. loss: 0.3568, \tAccuracy: 9202/10000 (92.02%)\n",
            "\n",
            "Training Epoch: 84 [====================] (60048/60000)\n",
            "Trained Epoch: 84 with loss= 0.375178, accuracy= 91.67, and epsilon= 9.05\n",
            "Test set metrics: \tAvg. loss: 0.3525, \tAccuracy: 9205/10000 (92.05%)\n",
            "\n",
            "Training Epoch: 85 [====================] (60379/60000)\n",
            "Trained Epoch: 85 with loss= 0.369982, accuracy= 92.19, and epsilon= 9.11\n",
            "Test set metrics: \tAvg. loss: 0.3521, \tAccuracy: 9217/10000 (92.17%)\n",
            "\n",
            "Training Epoch: 86 [====================] (60060/60000)\n",
            "Trained Epoch: 86 with loss= 0.372886, accuracy= 91.60, and epsilon= 9.17\n",
            "Test set metrics: \tAvg. loss: 0.3530, \tAccuracy: 9221/10000 (92.21%)\n",
            "\n",
            "Training Epoch: 87 [====================] (59961/60000)\n",
            "Trained Epoch: 87 with loss= 0.365113, accuracy= 91.58, and epsilon= 9.23\n",
            "Test set metrics: \tAvg. loss: 0.3485, \tAccuracy: 9238/10000 (92.38%)\n",
            "\n",
            "Training Epoch: 88 [====================] (60273/60000)\n",
            "Trained Epoch: 88 with loss= 0.357948, accuracy= 92.20, and epsilon= 9.29\n",
            "Test set metrics: \tAvg. loss: 0.3492, \tAccuracy: 9229/10000 (92.29%)\n",
            "\n",
            "Training Epoch: 89 [====================] (60386/60000)\n",
            "Trained Epoch: 89 with loss= 0.368269, accuracy= 92.35, and epsilon= 9.35\n",
            "Test set metrics: \tAvg. loss: 0.3514, \tAccuracy: 9231/10000 (92.31%)\n",
            "\n",
            "Training Epoch: 90 [====================] (60589/60000)\n",
            "Trained Epoch: 90 with loss= 0.364335, accuracy= 92.62, and epsilon= 9.41\n",
            "Test set metrics: \tAvg. loss: 0.3451, \tAccuracy: 9220/10000 (92.20%)\n",
            "\n",
            "Training Epoch: 91 [====================] (59843/60000)\n",
            "Trained Epoch: 91 with loss= 0.366922, accuracy= 91.43, and epsilon= 9.47\n",
            "Test set metrics: \tAvg. loss: 0.3471, \tAccuracy: 9230/10000 (92.30%)\n",
            "\n",
            "Training Epoch: 92 [====================] (60059/60000)\n",
            "Trained Epoch: 92 with loss= 0.368720, accuracy= 91.85, and epsilon= 9.53\n",
            "Test set metrics: \tAvg. loss: 0.3520, \tAccuracy: 9213/10000 (92.13%)\n",
            "\n",
            "Training Epoch: 93 [====================] (60120/60000)\n",
            "Trained Epoch: 93 with loss= 0.364252, accuracy= 92.00, and epsilon= 9.59\n",
            "Test set metrics: \tAvg. loss: 0.3458, \tAccuracy: 9247/10000 (92.47%)\n",
            "\n",
            "Training Epoch: 94 [====================] (60060/60000)\n",
            "Trained Epoch: 94 with loss= 0.369892, accuracy= 91.68, and epsilon= 9.65\n",
            "Test set metrics: \tAvg. loss: 0.3465, \tAccuracy: 9236/10000 (92.36%)\n",
            "\n",
            "Training Epoch: 95 [====================] (60219/60000)\n",
            "Trained Epoch: 95 with loss= 0.369758, accuracy= 92.12, and epsilon= 9.70\n",
            "Test set metrics: \tAvg. loss: 0.3495, \tAccuracy: 9226/10000 (92.26%)\n",
            "\n",
            "Training Epoch: 96 [====================] (60342/60000)\n",
            "Trained Epoch: 96 with loss= 0.361462, accuracy= 92.37, and epsilon= 9.76\n",
            "Test set metrics: \tAvg. loss: 0.3521, \tAccuracy: 9238/10000 (92.38%)\n",
            "\n",
            "Training Epoch: 97 [====================] (59803/60000)\n",
            "Trained Epoch: 97 with loss= 0.364501, accuracy= 91.48, and epsilon= 9.82\n",
            "Test set metrics: \tAvg. loss: 0.3519, \tAccuracy: 9235/10000 (92.35%)\n",
            "\n",
            "Training Epoch: 98 [====================] (59612/60000)\n",
            "Trained Epoch: 98 with loss= 0.372617, accuracy= 91.08, and epsilon= 9.88\n",
            "Test set metrics: \tAvg. loss: 0.3507, \tAccuracy: 9245/10000 (92.45%)\n",
            "\n",
            "Training Epoch: 99 [====================] (59977/60000)\n",
            "Trained Epoch: 99 with loss= 0.362911, accuracy= 91.91, and epsilon= 9.94\n",
            "Test set metrics: \tAvg. loss: 0.3474, \tAccuracy: 9238/10000 (92.38%)\n",
            "\n",
            "Training Epoch: 100 [====================] (60416/60000)\n",
            "Trained Epoch: 100 with loss= 0.359523, accuracy= 92.46, and epsilon= 9.99\n",
            "Test set metrics: \tAvg. loss: 0.3460, \tAccuracy: 9239/10000 (92.39%)\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.validators import ModuleValidator\n",
        "\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 600\n",
        "lr = 0.05\n",
        "momentum = 0.9    #SET THIS PARAM\n",
        "max_grad_norm = 4\n",
        "sigma = 0.80230   #SET THIS PARAM\n",
        "delta = 1e-5\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(MNISTDatasetTrain(), batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(MNISTDatasetTest(), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(60, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n",
        "    \n",
        "model = Net()\n",
        "model = model.to(device)\n",
        "errors = ModuleValidator.validate(model, strict=False)\n",
        "print(\"ERRORS: \", errors[-5:])\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "privacy_engine = PrivacyEngine()\n",
        "model, optimizer, train_loader = privacy_engine.make_private(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    noise_multiplier=sigma,\n",
        "    max_grad_norm=max_grad_norm\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier} and C={max_grad_norm}\")\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    samples = 0\n",
        "    batches = int(len(train_loader.dataset) / (batch_size*5))\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    print('Training Epoch: {} ['.format(epoch) + '-'*batches+']', end='\\r')\n",
        "    \n",
        "    for data, target in train_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, torch.flatten(target))\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        samples += len(data)\n",
        "        i = int(samples/(batch_size*5))\n",
        "        count += 1\n",
        "        print('Training Epoch: {} ['.format(epoch) +'='*(i)+ '-'*(batches-i)+'] ({}/{})'.format(samples, len(train_loader.dataset)), end='\\r')\n",
        "    print('Training Epoch: {} ['.format(epoch) + '='*batches+'] ({}/{})'.format(samples, len(train_loader.dataset)))\n",
        "    epsilon = privacy_engine.get_epsilon(delta)\n",
        "    print('Trained Epoch: {} with loss= {:.6f}, accuracy= {:.2f}, and epsilon= {:.2f}'.format(\n",
        "            epoch,\n",
        "            epoch_loss / count,\n",
        "            100. * correct / len(train_loader.dataset),\n",
        "            epsilon\n",
        "    ))\n",
        "    train_losses.append(epoch_loss / count)\n",
        "    train_accs.append( (100. * correct / len(train_loader.dataset)).cpu().numpy() )\n",
        "    # torch.save(model.state_dict(), './saved/model.pth')\n",
        "    # torch.save(optimizer.state_dict(), './saved/optimizer.pth')\n",
        "            \n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, torch.flatten(target)).item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "            count+=1\n",
        "    test_loss /= count\n",
        "    test_losses.append(test_loss)\n",
        "    test_accs.append( (100. * correct / len(test_loader.dataset)).cpu().numpy() )\n",
        "    print('Test set metrics: \\tAvg. loss: {:.4f}, \\tAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)\n",
        "    ))\n",
        "    \n",
        "    \n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "    \n",
        "fig = plt.figure()\n",
        "plt.plot(np.arange(1, 101, 1), train_losses, color='blue')\n",
        "plt.plot(np.arange(1, 101, 1), test_losses, color='red')\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O783mrK8QYrB"
      },
      "outputs": [],
      "source": [
        "# save_file = \"./saved/target_eps_{}_c_{}_epochs_{}_momentum_{}_{}\"\n",
        "    \n",
        "# with open(save_file.format(\n",
        "#         epsilon, max_grad_norm, epochs, momentum, 'train_losses.npy'\n",
        "#     ), 'wb') as f:\n",
        "#     np.save(f, np.array(train_losses))\n",
        "    \n",
        "# with open(save_file.format(\n",
        "#         epsilon, max_grad_norm, epochs, momentum, 'train_accs.npy'\n",
        "#     ), 'wb') as f:\n",
        "#     np.save(f, np.array(train_accs))\n",
        "\n",
        "# with open(save_file.format(\n",
        "#         epsilon, max_grad_norm, epochs, momentum, 'test_losses.npy'\n",
        "#     ), 'wb') as f:\n",
        "#     np.save(f, np.array(test_losses))\n",
        "    \n",
        "# with open(save_file.format(\n",
        "#         epsilon, max_grad_norm, epochs, momentum, 'test_accs.npy'\n",
        "#     ), 'wb') as f:\n",
        "#     np.save(f, np.array(test_accs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWLc3iwkRKQE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "fd8dfbd7152dd8c4ca17de43a6d187ca210c3070a27f09091bdc3f19e49cff5b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
