{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTRpH6YcQXvJ",
        "outputId": "0e7892d6-ce19-4809-8f26-5ef47839cbe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opacus in /usr/local/lib/python3.8/dist-packages (1.3.0)\n",
            "Requirement already satisfied: functorch in /usr/local/lib/python3.8/dist-packages (from opacus) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.8/dist-packages (from opacus) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from opacus) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.8/dist-packages (from opacus) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.8/dist-packages (from opacus) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8->opacus) (4.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install opacus"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c2-4YyQ1QYh7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "\n",
        "pca = PCA(n_components=60)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "minmax = MinMaxScaler()\n",
        "X_train_pca = minmax.fit_transform(X_train_pca)\n",
        "X_test_pca = minmax.transform(X_test_pca)\n",
        "\n",
        "if torch.cuda.is_available(): \n",
        "    dev = \"cuda:0\" \n",
        "else: \n",
        "    dev = \"cpu\" \n",
        "device = torch.device(dev)\n",
        "\n",
        "class MNISTDatasetTrain(Dataset):\n",
        "    def __init__(self):\n",
        "        self.X_train_pca = X_train_pca\n",
        "        self.y_train = y_train\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X_train_pca)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.Tensor(self.X_train_pca[idx]), torch.tensor([self.y_train[idx]])\n",
        "    \n",
        "class MNISTDatasetTest(Dataset):\n",
        "    def __init__(self):\n",
        "        self.X_test_pca = X_test_pca\n",
        "        self.y_test = y_test\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X_test_pca)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.Tensor(self.X_test_pca[idx]), torch.tensor([self.y_test[idx]])\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vpEhmCODQYk6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58fOPLXjSMRI",
        "outputId": "a4b75e79-80a8-4aa2-8c81-4a424a7f37bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.validators import ModuleValidator\n",
        "\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 600\n",
        "lr = 0.05\n",
        "momentum = 0\n",
        "max_grad_norm = 4\n",
        "epsilon = 1\n",
        "delta = 1e-5\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(MNISTDatasetTrain(), batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(MNISTDatasetTest(), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(60, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n",
        "    \n",
        "model = Net()\n",
        "model = model.to(device)\n",
        "errors = ModuleValidator.validate(model, strict=False)\n",
        "print(\"ERRORS: \", errors[-5:])\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "privacy_engine = PrivacyEngine()\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    epochs=epochs,\n",
        "    target_epsilon=epsilon,\n",
        "    target_delta=delta,\n",
        "    max_grad_norm=max_grad_norm\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier} and C={max_grad_norm}\")\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    samples = 0\n",
        "    batches = int(len(train_loader.dataset) / (batch_size*5))\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    print('Training Epoch: {} ['.format(epoch) + '-'*batches+']', end='\\r')\n",
        "    \n",
        "    for data, target in train_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, torch.flatten(target))\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        samples += len(data)\n",
        "        i = int(samples/(batch_size*5))\n",
        "        count += 1\n",
        "        print('Training Epoch: {} ['.format(epoch) +'='*(i)+ '-'*(batches-i)+'] ({}/{})'.format(samples, len(train_loader.dataset)), end='\\r')\n",
        "    print('Training Epoch: {} ['.format(epoch) + '='*batches+'] ({}/{})'.format(samples, len(train_loader.dataset)))\n",
        "    epsilon = privacy_engine.get_epsilon(delta)\n",
        "    print('Trained Epoch: {} with loss= {:.6f}, accuracy= {:.2f}, and epsilon= {:.2f}'.format(\n",
        "            epoch,\n",
        "            epoch_loss / count,\n",
        "            100. * correct / len(train_loader.dataset),\n",
        "            epsilon\n",
        "    ))\n",
        "    train_losses.append(epoch_loss / count)\n",
        "    train_accs.append( (100. * correct / len(train_loader.dataset)).cpu().numpy() )\n",
        "    # torch.save(model.state_dict(), './saved/model.pth')\n",
        "    # torch.save(optimizer.state_dict(), './saved/optimizer.pth')\n",
        "            \n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, torch.flatten(target)).item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "            count+=1\n",
        "    test_loss /= count\n",
        "    test_losses.append(test_loss)\n",
        "    test_accs.append( (100. * correct / len(test_loader.dataset)).cpu().numpy() )\n",
        "    print('Test set metrics: \\tAvg. loss: {:.4f}, \\tAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)\n",
        "    ))\n",
        "    \n",
        "    \n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "    \n",
        "fig = plt.figure()\n",
        "plt.plot(np.arange(1, 101, 1), train_losses, color='blue')\n",
        "plt.plot(np.arange(1, 101, 1), test_losses, color='red')\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wBCWvIfXQYno",
        "outputId": "5ff08014-f439-4426-c7c2-0fade13c001c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERRORS:  []\n",
            "Using sigma=3.8671875 and C=4\n",
            "Training Epoch: 1 [====================] (59357/60000)\n",
            "Trained Epoch: 1 with loss= 2.258827, accuracy= 23.60, and epsilon= 0.09\n",
            "Test set metrics: \tAvg. loss: 2.2084, \tAccuracy: 2757/10000 (27.57%)\n",
            "\n",
            "Training Epoch: 2 [====================] (60067/60000)\n",
            "Trained Epoch: 2 with loss= 2.164483, accuracy= 44.51, and epsilon= 0.13\n",
            "Test set metrics: \tAvg. loss: 2.1124, \tAccuracy: 6019/10000 (60.19%)\n",
            "\n",
            "Training Epoch: 3 [====================] (59381/60000)\n",
            "Trained Epoch: 3 with loss= 2.065623, accuracy= 60.88, and epsilon= 0.16\n",
            "Test set metrics: \tAvg. loss: 2.0075, \tAccuracy: 6706/10000 (67.06%)\n",
            "\n",
            "Training Epoch: 4 [====================] (59643/60000)\n",
            "Trained Epoch: 4 with loss= 1.959196, accuracy= 67.31, and epsilon= 0.18\n",
            "Test set metrics: \tAvg. loss: 1.8956, \tAccuracy: 7077/10000 (70.77%)\n",
            "\n",
            "Training Epoch: 5 [====================] (59619/60000)\n",
            "Trained Epoch: 5 with loss= 1.842064, accuracy= 71.86, and epsilon= 0.21\n",
            "Test set metrics: \tAvg. loss: 1.7765, \tAccuracy: 7167/10000 (71.67%)\n",
            "\n",
            "Training Epoch: 6 [====================] (59561/60000)\n",
            "Trained Epoch: 6 with loss= 1.730713, accuracy= 73.91, and epsilon= 0.23\n",
            "Test set metrics: \tAvg. loss: 1.6608, \tAccuracy: 7676/10000 (76.76%)\n",
            "\n",
            "Training Epoch: 7 [====================] (60082/60000)\n",
            "Trained Epoch: 7 with loss= 1.613822, accuracy= 75.82, and epsilon= 0.24\n",
            "Test set metrics: \tAvg. loss: 1.5419, \tAccuracy: 7858/10000 (78.58%)\n",
            "\n",
            "Training Epoch: 8 [====================] (59956/60000)\n",
            "Trained Epoch: 8 with loss= 1.497276, accuracy= 77.40, and epsilon= 0.26\n",
            "Test set metrics: \tAvg. loss: 1.4211, \tAccuracy: 7978/10000 (79.78%)\n",
            "\n",
            "Training Epoch: 9 [====================] (60660/60000)\n",
            "Trained Epoch: 9 with loss= 1.373522, accuracy= 78.91, and epsilon= 0.28\n",
            "Test set metrics: \tAvg. loss: 1.2998, \tAccuracy: 8061/10000 (80.61%)\n",
            "\n",
            "Training Epoch: 10 [====================] (60224/60000)\n",
            "Trained Epoch: 10 with loss= 1.267323, accuracy= 79.39, and epsilon= 0.29\n",
            "Test set metrics: \tAvg. loss: 1.1962, \tAccuracy: 8043/10000 (80.43%)\n",
            "\n",
            "Training Epoch: 11 [====================] (60120/60000)\n",
            "Trained Epoch: 11 with loss= 1.165506, accuracy= 79.96, and epsilon= 0.31\n",
            "Test set metrics: \tAvg. loss: 1.1011, \tAccuracy: 8114/10000 (81.14%)\n",
            "\n",
            "Training Epoch: 12 [====================] (60189/60000)\n",
            "Trained Epoch: 12 with loss= 1.075616, accuracy= 80.69, and epsilon= 0.32\n",
            "Test set metrics: \tAvg. loss: 1.0170, \tAccuracy: 8104/10000 (81.04%)\n",
            "\n",
            "Training Epoch: 13 [====================] (59715/60000)\n",
            "Trained Epoch: 13 with loss= 0.995895, accuracy= 80.96, and epsilon= 0.34\n",
            "Test set metrics: \tAvg. loss: 0.9421, \tAccuracy: 8220/10000 (82.20%)\n",
            "\n",
            "Training Epoch: 14 [====================] (60366/60000)\n",
            "Trained Epoch: 14 with loss= 0.927157, accuracy= 82.51, and epsilon= 0.35\n",
            "Test set metrics: \tAvg. loss: 0.8724, \tAccuracy: 8348/10000 (83.48%)\n",
            "\n",
            "Training Epoch: 15 [====================] (59859/60000)\n",
            "Trained Epoch: 15 with loss= 0.861305, accuracy= 82.52, and epsilon= 0.36\n",
            "Test set metrics: \tAvg. loss: 0.8118, \tAccuracy: 8406/10000 (84.06%)\n",
            "\n",
            "Training Epoch: 16 [====================] (60095/60000)\n",
            "Trained Epoch: 16 with loss= 0.805959, accuracy= 83.61, and epsilon= 0.37\n",
            "Test set metrics: \tAvg. loss: 0.7623, \tAccuracy: 8421/10000 (84.21%)\n",
            "\n",
            "Training Epoch: 17 [====================] (59552/60000)\n",
            "Trained Epoch: 17 with loss= 0.763595, accuracy= 83.42, and epsilon= 0.39\n",
            "Test set metrics: \tAvg. loss: 0.7161, \tAccuracy: 8542/10000 (85.42%)\n",
            "\n",
            "Training Epoch: 18 [====================] (60228/60000)\n",
            "Trained Epoch: 18 with loss= 0.720331, accuracy= 84.87, and epsilon= 0.40\n",
            "Test set metrics: \tAvg. loss: 0.6777, \tAccuracy: 8564/10000 (85.64%)\n",
            "\n",
            "Training Epoch: 19 [====================] (59763/60000)\n",
            "Trained Epoch: 19 with loss= 0.684863, accuracy= 84.43, and epsilon= 0.41\n",
            "Test set metrics: \tAvg. loss: 0.6446, \tAccuracy: 8618/10000 (86.18%)\n",
            "\n",
            "Training Epoch: 20 [====================] (60072/60000)\n",
            "Trained Epoch: 20 with loss= 0.657156, accuracy= 85.19, and epsilon= 0.42\n",
            "Test set metrics: \tAvg. loss: 0.6168, \tAccuracy: 8647/10000 (86.47%)\n",
            "\n",
            "Training Epoch: 21 [====================] (59730/60000)\n",
            "Trained Epoch: 21 with loss= 0.632248, accuracy= 84.86, and epsilon= 0.43\n",
            "Test set metrics: \tAvg. loss: 0.5914, \tAccuracy: 8668/10000 (86.68%)\n",
            "\n",
            "Training Epoch: 22 [====================] (59974/60000)\n",
            "Trained Epoch: 22 with loss= 0.604292, accuracy= 85.39, and epsilon= 0.44\n",
            "Test set metrics: \tAvg. loss: 0.5713, \tAccuracy: 8685/10000 (86.85%)\n",
            "\n",
            "Training Epoch: 23 [====================] (59486/60000)\n",
            "Trained Epoch: 23 with loss= 0.588799, accuracy= 84.95, and epsilon= 0.45\n",
            "Test set metrics: \tAvg. loss: 0.5530, \tAccuracy: 8670/10000 (86.70%)\n",
            "\n",
            "Training Epoch: 24 [====================] (59551/60000)\n",
            "Trained Epoch: 24 with loss= 0.568043, accuracy= 85.29, and epsilon= 0.46\n",
            "Test set metrics: \tAvg. loss: 0.5352, \tAccuracy: 8686/10000 (86.86%)\n",
            "\n",
            "Training Epoch: 25 [====================] (59713/60000)\n",
            "Trained Epoch: 25 with loss= 0.555703, accuracy= 85.65, and epsilon= 0.47\n",
            "Test set metrics: \tAvg. loss: 0.5192, \tAccuracy: 8710/10000 (87.10%)\n",
            "\n",
            "Training Epoch: 26 [====================] (59951/60000)\n",
            "Trained Epoch: 26 with loss= 0.538746, accuracy= 85.91, and epsilon= 0.48\n",
            "Test set metrics: \tAvg. loss: 0.5047, \tAccuracy: 8731/10000 (87.31%)\n",
            "\n",
            "Training Epoch: 27 [====================] (59955/60000)\n",
            "Trained Epoch: 27 with loss= 0.520939, accuracy= 86.32, and epsilon= 0.49\n",
            "Test set metrics: \tAvg. loss: 0.4942, \tAccuracy: 8746/10000 (87.46%)\n",
            "\n",
            "Training Epoch: 28 [====================] (59799/60000)\n",
            "Trained Epoch: 28 with loss= 0.519723, accuracy= 85.87, and epsilon= 0.50\n",
            "Test set metrics: \tAvg. loss: 0.4825, \tAccuracy: 8767/10000 (87.67%)\n",
            "\n",
            "Training Epoch: 29 [====================] (59923/60000)\n",
            "Trained Epoch: 29 with loss= 0.503066, accuracy= 86.50, and epsilon= 0.51\n",
            "Test set metrics: \tAvg. loss: 0.4731, \tAccuracy: 8769/10000 (87.69%)\n",
            "\n",
            "Training Epoch: 30 [====================] (60195/60000)\n",
            "Trained Epoch: 30 with loss= 0.494053, accuracy= 86.91, and epsilon= 0.52\n",
            "Test set metrics: \tAvg. loss: 0.4640, \tAccuracy: 8772/10000 (87.72%)\n",
            "\n",
            "Training Epoch: 31 [====================] (59930/60000)\n",
            "Trained Epoch: 31 with loss= 0.477578, accuracy= 87.03, and epsilon= 0.53\n",
            "Test set metrics: \tAvg. loss: 0.4541, \tAccuracy: 8793/10000 (87.93%)\n",
            "\n",
            "Training Epoch: 32 [====================] (60252/60000)\n",
            "Trained Epoch: 32 with loss= 0.475727, accuracy= 87.36, and epsilon= 0.54\n",
            "Test set metrics: \tAvg. loss: 0.4471, \tAccuracy: 8788/10000 (87.88%)\n",
            "\n",
            "Training Epoch: 33 [====================] (60352/60000)\n",
            "Trained Epoch: 33 with loss= 0.473077, accuracy= 87.42, and epsilon= 0.55\n",
            "Test set metrics: \tAvg. loss: 0.4383, \tAccuracy: 8819/10000 (88.19%)\n",
            "\n",
            "Training Epoch: 34 [====================] (60017/60000)\n",
            "Trained Epoch: 34 with loss= 0.460203, accuracy= 87.31, and epsilon= 0.56\n",
            "Test set metrics: \tAvg. loss: 0.4328, \tAccuracy: 8828/10000 (88.28%)\n",
            "\n",
            "Training Epoch: 35 [====================] (59996/60000)\n",
            "Trained Epoch: 35 with loss= 0.456655, accuracy= 87.39, and epsilon= 0.57\n",
            "Test set metrics: \tAvg. loss: 0.4305, \tAccuracy: 8816/10000 (88.16%)\n",
            "\n",
            "Training Epoch: 36 [====================] (59539/60000)\n",
            "Trained Epoch: 36 with loss= 0.453171, accuracy= 86.55, and epsilon= 0.57\n",
            "Test set metrics: \tAvg. loss: 0.4267, \tAccuracy: 8824/10000 (88.24%)\n",
            "\n",
            "Training Epoch: 37 [====================] (60021/60000)\n",
            "Trained Epoch: 37 with loss= 0.445824, accuracy= 87.36, and epsilon= 0.58\n",
            "Test set metrics: \tAvg. loss: 0.4175, \tAccuracy: 8853/10000 (88.53%)\n",
            "\n",
            "Training Epoch: 38 [====================] (59348/60000)\n",
            "Trained Epoch: 38 with loss= 0.450704, accuracy= 86.15, and epsilon= 0.59\n",
            "Test set metrics: \tAvg. loss: 0.4138, \tAccuracy: 8868/10000 (88.68%)\n",
            "\n",
            "Training Epoch: 39 [====================] (59725/60000)\n",
            "Trained Epoch: 39 with loss= 0.439956, accuracy= 87.12, and epsilon= 0.60\n",
            "Test set metrics: \tAvg. loss: 0.4087, \tAccuracy: 8860/10000 (88.60%)\n",
            "\n",
            "Training Epoch: 40 [====================] (60221/60000)\n",
            "Trained Epoch: 40 with loss= 0.432759, accuracy= 87.86, and epsilon= 0.61\n",
            "Test set metrics: \tAvg. loss: 0.4062, \tAccuracy: 8865/10000 (88.65%)\n",
            "\n",
            "Training Epoch: 41 [====================] (60084/60000)\n",
            "Trained Epoch: 41 with loss= 0.435010, accuracy= 87.61, and epsilon= 0.62\n",
            "Test set metrics: \tAvg. loss: 0.4036, \tAccuracy: 8869/10000 (88.69%)\n",
            "\n",
            "Training Epoch: 42 [====================] (60374/60000)\n",
            "Trained Epoch: 42 with loss= 0.424536, accuracy= 88.30, and epsilon= 0.62\n",
            "Test set metrics: \tAvg. loss: 0.3961, \tAccuracy: 8882/10000 (88.82%)\n",
            "\n",
            "Training Epoch: 43 [====================] (60171/60000)\n",
            "Trained Epoch: 43 with loss= 0.424531, accuracy= 88.13, and epsilon= 0.63\n",
            "Test set metrics: \tAvg. loss: 0.3940, \tAccuracy: 8883/10000 (88.83%)\n",
            "\n",
            "Training Epoch: 44 [====================] (59863/60000)\n",
            "Trained Epoch: 44 with loss= 0.425352, accuracy= 87.72, and epsilon= 0.64\n",
            "Test set metrics: \tAvg. loss: 0.3908, \tAccuracy: 8888/10000 (88.88%)\n",
            "\n",
            "Training Epoch: 45 [====================] (59743/60000)\n",
            "Trained Epoch: 45 with loss= 0.417727, accuracy= 87.43, and epsilon= 0.65\n",
            "Test set metrics: \tAvg. loss: 0.3895, \tAccuracy: 8886/10000 (88.86%)\n",
            "\n",
            "Training Epoch: 46 [====================] (59762/60000)\n",
            "Trained Epoch: 46 with loss= 0.415232, accuracy= 87.79, and epsilon= 0.65\n",
            "Test set metrics: \tAvg. loss: 0.3861, \tAccuracy: 8886/10000 (88.86%)\n",
            "\n",
            "Training Epoch: 47 [====================] (59922/60000)\n",
            "Trained Epoch: 47 with loss= 0.413808, accuracy= 87.95, and epsilon= 0.66\n",
            "Test set metrics: \tAvg. loss: 0.3846, \tAccuracy: 8893/10000 (88.93%)\n",
            "\n",
            "Training Epoch: 48 [====================] (60415/60000)\n",
            "Trained Epoch: 48 with loss= 0.412415, accuracy= 88.65, and epsilon= 0.67\n",
            "Test set metrics: \tAvg. loss: 0.3828, \tAccuracy: 8890/10000 (88.90%)\n",
            "\n",
            "Training Epoch: 49 [====================] (60129/60000)\n",
            "Trained Epoch: 49 with loss= 0.411795, accuracy= 88.08, and epsilon= 0.68\n",
            "Test set metrics: \tAvg. loss: 0.3805, \tAccuracy: 8900/10000 (89.00%)\n",
            "\n",
            "Training Epoch: 50 [====================] (60266/60000)\n",
            "Trained Epoch: 50 with loss= 0.417308, accuracy= 88.29, and epsilon= 0.68\n",
            "Test set metrics: \tAvg. loss: 0.3789, \tAccuracy: 8912/10000 (89.12%)\n",
            "\n",
            "Training Epoch: 51 [====================] (60009/60000)\n",
            "Trained Epoch: 51 with loss= 0.403086, accuracy= 88.40, and epsilon= 0.69\n",
            "Test set metrics: \tAvg. loss: 0.3791, \tAccuracy: 8921/10000 (89.21%)\n",
            "\n",
            "Training Epoch: 52 [====================] (59727/60000)\n",
            "Trained Epoch: 52 with loss= 0.403412, accuracy= 87.90, and epsilon= 0.70\n",
            "Test set metrics: \tAvg. loss: 0.3762, \tAccuracy: 8905/10000 (89.05%)\n",
            "\n",
            "Training Epoch: 53 [====================] (59945/60000)\n",
            "Trained Epoch: 53 with loss= 0.401328, accuracy= 88.14, and epsilon= 0.71\n",
            "Test set metrics: \tAvg. loss: 0.3725, \tAccuracy: 8907/10000 (89.07%)\n",
            "\n",
            "Training Epoch: 54 [====================] (59349/60000)\n",
            "Trained Epoch: 54 with loss= 0.407847, accuracy= 87.21, and epsilon= 0.71\n",
            "Test set metrics: \tAvg. loss: 0.3731, \tAccuracy: 8927/10000 (89.27%)\n",
            "\n",
            "Training Epoch: 55 [====================] (60551/60000)\n",
            "Trained Epoch: 55 with loss= 0.402550, accuracy= 89.04, and epsilon= 0.72\n",
            "Test set metrics: \tAvg. loss: 0.3738, \tAccuracy: 8935/10000 (89.35%)\n",
            "\n",
            "Training Epoch: 56 [====================] (60134/60000)\n",
            "Trained Epoch: 56 with loss= 0.397767, accuracy= 88.58, and epsilon= 0.73\n",
            "Test set metrics: \tAvg. loss: 0.3724, \tAccuracy: 8932/10000 (89.32%)\n",
            "\n",
            "Training Epoch: 57 [====================] (59779/60000)\n",
            "Trained Epoch: 57 with loss= 0.399203, accuracy= 88.02, and epsilon= 0.73\n",
            "Test set metrics: \tAvg. loss: 0.3700, \tAccuracy: 8938/10000 (89.38%)\n",
            "\n",
            "Training Epoch: 58 [====================] (59889/60000)\n",
            "Trained Epoch: 58 with loss= 0.391692, accuracy= 88.31, and epsilon= 0.74\n",
            "Test set metrics: \tAvg. loss: 0.3693, \tAccuracy: 8931/10000 (89.31%)\n",
            "\n",
            "Training Epoch: 59 [====================] (59916/60000)\n",
            "Trained Epoch: 59 with loss= 0.397362, accuracy= 88.18, and epsilon= 0.75\n",
            "Test set metrics: \tAvg. loss: 0.3678, \tAccuracy: 8932/10000 (89.32%)\n",
            "\n",
            "Training Epoch: 60 [====================] (59715/60000)\n",
            "Trained Epoch: 60 with loss= 0.389519, accuracy= 88.18, and epsilon= 0.75\n",
            "Test set metrics: \tAvg. loss: 0.3671, \tAccuracy: 8939/10000 (89.39%)\n",
            "\n",
            "Training Epoch: 61 [====================] (60202/60000)\n",
            "Trained Epoch: 61 with loss= 0.389191, accuracy= 88.85, and epsilon= 0.76\n",
            "Test set metrics: \tAvg. loss: 0.3655, \tAccuracy: 8939/10000 (89.39%)\n",
            "\n",
            "Training Epoch: 62 [====================] (59820/60000)\n",
            "Trained Epoch: 62 with loss= 0.392847, accuracy= 88.28, and epsilon= 0.77\n",
            "Test set metrics: \tAvg. loss: 0.3644, \tAccuracy: 8944/10000 (89.44%)\n",
            "\n",
            "Training Epoch: 63 [====================] (60193/60000)\n",
            "Trained Epoch: 63 with loss= 0.392731, accuracy= 88.75, and epsilon= 0.77\n",
            "Test set metrics: \tAvg. loss: 0.3629, \tAccuracy: 8941/10000 (89.41%)\n",
            "\n",
            "Training Epoch: 64 [====================] (59886/60000)\n",
            "Trained Epoch: 64 with loss= 0.385105, accuracy= 88.53, and epsilon= 0.78\n",
            "Test set metrics: \tAvg. loss: 0.3652, \tAccuracy: 8937/10000 (89.37%)\n",
            "\n",
            "Training Epoch: 65 [====================] (59671/60000)\n",
            "Trained Epoch: 65 with loss= 0.387733, accuracy= 88.04, and epsilon= 0.79\n",
            "Test set metrics: \tAvg. loss: 0.3617, \tAccuracy: 8950/10000 (89.50%)\n",
            "\n",
            "Training Epoch: 66 [====================] (59973/60000)\n",
            "Trained Epoch: 66 with loss= 0.381422, accuracy= 88.67, and epsilon= 0.79\n",
            "Test set metrics: \tAvg. loss: 0.3625, \tAccuracy: 8956/10000 (89.56%)\n",
            "\n",
            "Training Epoch: 67 [====================] (60120/60000)\n",
            "Trained Epoch: 67 with loss= 0.383923, accuracy= 88.98, and epsilon= 0.80\n",
            "Test set metrics: \tAvg. loss: 0.3617, \tAccuracy: 8942/10000 (89.42%)\n",
            "\n",
            "Training Epoch: 68 [====================] (60002/60000)\n",
            "Trained Epoch: 68 with loss= 0.388124, accuracy= 88.71, and epsilon= 0.81\n",
            "Test set metrics: \tAvg. loss: 0.3603, \tAccuracy: 8961/10000 (89.61%)\n",
            "\n",
            "Training Epoch: 69 [====================] (60120/60000)\n",
            "Trained Epoch: 69 with loss= 0.386660, accuracy= 88.93, and epsilon= 0.81\n",
            "Test set metrics: \tAvg. loss: 0.3597, \tAccuracy: 8954/10000 (89.54%)\n",
            "\n",
            "Training Epoch: 70 [====================] (59932/60000)\n",
            "Trained Epoch: 70 with loss= 0.394625, accuracy= 88.47, and epsilon= 0.82\n",
            "Test set metrics: \tAvg. loss: 0.3639, \tAccuracy: 8952/10000 (89.52%)\n",
            "\n",
            "Training Epoch: 71 [====================] (60365/60000)\n",
            "Trained Epoch: 71 with loss= 0.384777, accuracy= 89.31, and epsilon= 0.83\n",
            "Test set metrics: \tAvg. loss: 0.3606, \tAccuracy: 8954/10000 (89.54%)\n",
            "\n",
            "Training Epoch: 72 [====================] (59826/60000)\n",
            "Trained Epoch: 72 with loss= 0.384758, accuracy= 88.56, and epsilon= 0.83\n",
            "Test set metrics: \tAvg. loss: 0.3602, \tAccuracy: 8952/10000 (89.52%)\n",
            "\n",
            "Training Epoch: 73 [====================] (60045/60000)\n",
            "Trained Epoch: 73 with loss= 0.383853, accuracy= 88.79, and epsilon= 0.84\n",
            "Test set metrics: \tAvg. loss: 0.3582, \tAccuracy: 8948/10000 (89.48%)\n",
            "\n",
            "Training Epoch: 74 [====================] (59712/60000)\n",
            "Trained Epoch: 74 with loss= 0.384865, accuracy= 88.28, and epsilon= 0.84\n",
            "Test set metrics: \tAvg. loss: 0.3581, \tAccuracy: 8954/10000 (89.54%)\n",
            "\n",
            "Training Epoch: 75 [====================] (59790/60000)\n",
            "Trained Epoch: 75 with loss= 0.386871, accuracy= 88.39, and epsilon= 0.85\n",
            "Test set metrics: \tAvg. loss: 0.3563, \tAccuracy: 8960/10000 (89.60%)\n",
            "\n",
            "Training Epoch: 76 [====================] (59717/60000)\n",
            "Trained Epoch: 76 with loss= 0.393255, accuracy= 88.35, and epsilon= 0.86\n",
            "Test set metrics: \tAvg. loss: 0.3578, \tAccuracy: 8971/10000 (89.71%)\n",
            "\n",
            "Training Epoch: 77 [====================] (60111/60000)\n",
            "Trained Epoch: 77 with loss= 0.385255, accuracy= 88.93, and epsilon= 0.86\n",
            "Test set metrics: \tAvg. loss: 0.3579, \tAccuracy: 8951/10000 (89.51%)\n",
            "\n",
            "Training Epoch: 78 [====================] (59623/60000)\n",
            "Trained Epoch: 78 with loss= 0.384325, accuracy= 88.33, and epsilon= 0.87\n",
            "Test set metrics: \tAvg. loss: 0.3548, \tAccuracy: 8949/10000 (89.49%)\n",
            "\n",
            "Training Epoch: 79 [====================] (59957/60000)\n",
            "Trained Epoch: 79 with loss= 0.385355, accuracy= 88.66, and epsilon= 0.88\n",
            "Test set metrics: \tAvg. loss: 0.3539, \tAccuracy: 8966/10000 (89.66%)\n",
            "\n",
            "Training Epoch: 80 [====================] (59984/60000)\n",
            "Trained Epoch: 80 with loss= 0.377646, accuracy= 88.89, and epsilon= 0.88\n",
            "Test set metrics: \tAvg. loss: 0.3538, \tAccuracy: 8963/10000 (89.63%)\n",
            "\n",
            "Training Epoch: 81 [====================] (60157/60000)\n",
            "Trained Epoch: 81 with loss= 0.381968, accuracy= 89.05, and epsilon= 0.89\n",
            "Test set metrics: \tAvg. loss: 0.3535, \tAccuracy: 8968/10000 (89.68%)\n",
            "\n",
            "Training Epoch: 82 [====================] (59988/60000)\n",
            "Trained Epoch: 82 with loss= 0.378719, accuracy= 88.93, and epsilon= 0.89\n",
            "Test set metrics: \tAvg. loss: 0.3540, \tAccuracy: 8964/10000 (89.64%)\n",
            "\n",
            "Training Epoch: 83 [====================] (59728/60000)\n",
            "Trained Epoch: 83 with loss= 0.384781, accuracy= 88.39, and epsilon= 0.90\n",
            "Test set metrics: \tAvg. loss: 0.3509, \tAccuracy: 8968/10000 (89.68%)\n",
            "\n",
            "Training Epoch: 84 [====================] (60029/60000)\n",
            "Trained Epoch: 84 with loss= 0.382045, accuracy= 89.05, and epsilon= 0.90\n",
            "Test set metrics: \tAvg. loss: 0.3518, \tAccuracy: 8971/10000 (89.71%)\n",
            "\n",
            "Training Epoch: 85 [====================] (60220/60000)\n",
            "Trained Epoch: 85 with loss= 0.386620, accuracy= 89.24, and epsilon= 0.91\n",
            "Test set metrics: \tAvg. loss: 0.3519, \tAccuracy: 8962/10000 (89.62%)\n",
            "\n",
            "Training Epoch: 86 [====================] (59931/60000)\n",
            "Trained Epoch: 86 with loss= 0.380914, accuracy= 88.80, and epsilon= 0.92\n",
            "Test set metrics: \tAvg. loss: 0.3523, \tAccuracy: 8977/10000 (89.77%)\n",
            "\n",
            "Training Epoch: 87 [====================] (60051/60000)\n",
            "Trained Epoch: 87 with loss= 0.390727, accuracy= 88.83, and epsilon= 0.92\n",
            "Test set metrics: \tAvg. loss: 0.3506, \tAccuracy: 8959/10000 (89.59%)\n",
            "\n",
            "Training Epoch: 88 [====================] (59724/60000)\n",
            "Trained Epoch: 88 with loss= 0.383239, accuracy= 88.44, and epsilon= 0.93\n",
            "Test set metrics: \tAvg. loss: 0.3530, \tAccuracy: 8964/10000 (89.64%)\n",
            "\n",
            "Training Epoch: 89 [====================] (59768/60000)\n",
            "Trained Epoch: 89 with loss= 0.384175, accuracy= 88.73, and epsilon= 0.93\n",
            "Test set metrics: \tAvg. loss: 0.3522, \tAccuracy: 8972/10000 (89.72%)\n",
            "\n",
            "Training Epoch: 90 [====================] (60187/60000)\n",
            "Trained Epoch: 90 with loss= 0.378837, accuracy= 89.37, and epsilon= 0.94\n",
            "Test set metrics: \tAvg. loss: 0.3531, \tAccuracy: 8969/10000 (89.69%)\n",
            "\n",
            "Training Epoch: 91 [====================] (59615/60000)\n",
            "Trained Epoch: 91 with loss= 0.375986, accuracy= 88.54, and epsilon= 0.94\n",
            "Test set metrics: \tAvg. loss: 0.3516, \tAccuracy: 8968/10000 (89.68%)\n",
            "\n",
            "Training Epoch: 92 [====================] (60235/60000)\n",
            "Trained Epoch: 92 with loss= 0.379809, accuracy= 89.44, and epsilon= 0.95\n",
            "Test set metrics: \tAvg. loss: 0.3494, \tAccuracy: 8978/10000 (89.78%)\n",
            "\n",
            "Training Epoch: 93 [====================] (59742/60000)\n",
            "Trained Epoch: 93 with loss= 0.373311, accuracy= 88.82, and epsilon= 0.96\n",
            "Test set metrics: \tAvg. loss: 0.3536, \tAccuracy: 8971/10000 (89.71%)\n",
            "\n",
            "Training Epoch: 94 [====================] (59978/60000)\n",
            "Trained Epoch: 94 with loss= 0.384716, accuracy= 88.93, and epsilon= 0.96\n",
            "Test set metrics: \tAvg. loss: 0.3544, \tAccuracy: 8969/10000 (89.69%)\n",
            "\n",
            "Training Epoch: 95 [====================] (59801/60000)\n",
            "Trained Epoch: 95 with loss= 0.383526, accuracy= 88.77, and epsilon= 0.97\n",
            "Test set metrics: \tAvg. loss: 0.3523, \tAccuracy: 8972/10000 (89.72%)\n",
            "\n",
            "Training Epoch: 96 [====================] (60208/60000)\n",
            "Trained Epoch: 96 with loss= 0.376167, accuracy= 89.51, and epsilon= 0.97\n",
            "Test set metrics: \tAvg. loss: 0.3536, \tAccuracy: 8979/10000 (89.79%)\n",
            "\n",
            "Training Epoch: 97 [====================] (60036/60000)\n",
            "Trained Epoch: 97 with loss= 0.376783, accuracy= 89.24, and epsilon= 0.98\n",
            "Test set metrics: \tAvg. loss: 0.3510, \tAccuracy: 8978/10000 (89.78%)\n",
            "\n",
            "Training Epoch: 98 [====================] (59596/60000)\n",
            "Trained Epoch: 98 with loss= 0.378598, accuracy= 88.65, and epsilon= 0.98\n",
            "Test set metrics: \tAvg. loss: 0.3504, \tAccuracy: 8975/10000 (89.75%)\n",
            "\n",
            "Training Epoch: 99 [====================] (60507/60000)\n",
            "Trained Epoch: 99 with loss= 0.386389, accuracy= 89.79, and epsilon= 0.99\n",
            "Test set metrics: \tAvg. loss: 0.3511, \tAccuracy: 8987/10000 (89.87%)\n",
            "\n",
            "Training Epoch: 100 [====================] (59797/60000)\n",
            "Trained Epoch: 100 with loss= 0.378921, accuracy= 88.95, and epsilon= 0.99\n",
            "Test set metrics: \tAvg. loss: 0.3500, \tAccuracy: 8970/10000 (89.70%)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_file = \"./saved/target_eps_{}_c_{}_epochs_{}_momentum_{}_{}\"\n",
        "    \n",
        "with open(save_file.format(\n",
        "        epsilon, max_grad_norm, epochs, momentum, 'train_losses.npy'\n",
        "    ), 'wb') as f:\n",
        "    np.save(f, np.array(train_losses))\n",
        "    \n",
        "with open(save_file.format(\n",
        "        epsilon, max_grad_norm, epochs, momentum, 'train_accs.npy'\n",
        "    ), 'wb') as f:\n",
        "    np.save(f, np.array(train_accs))\n",
        "\n",
        "with open(save_file.format(\n",
        "        epsilon, max_grad_norm, epochs, momentum, 'test_losses.npy'\n",
        "    ), 'wb') as f:\n",
        "    np.save(f, np.array(test_losses))\n",
        "    \n",
        "with open(save_file.format(\n",
        "        epsilon, max_grad_norm, epochs, momentum, 'test_accs.npy'\n",
        "    ), 'wb') as f:\n",
        "    np.save(f, np.array(test_accs))"
      ],
      "metadata": {
        "id": "O783mrK8QYrB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OWLc3iwkRKQE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}