{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTRpH6YcQXvJ",
        "outputId": "807df0c9-5639-4ca4-895f-198d6b199930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opacus\n",
            "  Downloading opacus-1.3.0-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.9/216.9 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting functorch\n",
            "  Downloading functorch-1.13.1-py2.py3-none-any.whl (2.1 kB)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from opacus) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.8/dist-packages (from opacus) (3.3.0)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.8/dist-packages (from opacus) (1.13.1+cu116)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.8/dist-packages (from opacus) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8->opacus) (4.4.0)\n",
            "Installing collected packages: functorch, opacus\n",
            "Successfully installed functorch-1.13.1 opacus-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install opacus"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c2-4YyQ1QYh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "\n",
        "pca = PCA(n_components=60)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "minmax = MinMaxScaler()\n",
        "X_train_pca = minmax.fit_transform(X_train_pca)\n",
        "X_test_pca = minmax.transform(X_test_pca)\n",
        "\n",
        "if torch.cuda.is_available(): \n",
        "    dev = \"cuda:0\" \n",
        "else: \n",
        "    dev = \"cpu\" \n",
        "device = torch.device(dev)\n",
        "\n",
        "class MNISTDatasetTrain(Dataset):\n",
        "    def __init__(self):\n",
        "        self.X_train_pca = X_train_pca\n",
        "        self.y_train = y_train\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X_train_pca)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.Tensor(self.X_train_pca[idx]), torch.tensor([self.y_train[idx]])\n",
        "    \n",
        "class MNISTDatasetTest(Dataset):\n",
        "    def __init__(self):\n",
        "        self.X_test_pca = X_test_pca\n",
        "        self.y_test = y_test\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X_test_pca)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.Tensor(self.X_test_pca[idx]), torch.tensor([self.y_test[idx]])\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vpEhmCODQYk6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58fOPLXjSMRI",
        "outputId": "d92e1182-7c99-4656-9450-a5e50c13da29"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.validators import ModuleValidator\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 600\n",
        "lr = 0.05\n",
        "momentum = 0.9\n",
        "max_grad_norm = 4\n",
        "epsilon = 10\n",
        "delta = 1e-5\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(MNISTDatasetTrain(), batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(MNISTDatasetTest(), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(60, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n",
        "    \n",
        "model = Net()\n",
        "model = model.to(device)\n",
        "errors = ModuleValidator.validate(model, strict=False)\n",
        "print(\"ERRORS: \", errors[-5:])\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "privacy_engine = PrivacyEngine()\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    epochs=epochs,\n",
        "    target_epsilon=epsilon,\n",
        "    target_delta=delta,\n",
        "    max_grad_norm=max_grad_norm\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier} and C={max_grad_norm}\")\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    samples = 0\n",
        "    batches = int(len(train_loader.dataset) / (batch_size*5))\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    print('Training Epoch: {} ['.format(epoch) + '-'*batches+']', end='\\r')\n",
        "    for data, target in train_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, torch.flatten(target))\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        samples += len(data)\n",
        "        i = int(samples/(batch_size*5))\n",
        "        count += 1\n",
        "        print('Training Epoch: {} ['.format(epoch) +'='*(i)+ '-'*(batches-i)+'] ({}/{})'.format(samples, len(train_loader.dataset)), end='\\r')\n",
        "    print('Training Epoch: {} ['.format(epoch) + '='*batches+'] ({}/{})'.format(samples, len(train_loader.dataset)))\n",
        "    epsilon = privacy_engine.get_epsilon(delta)\n",
        "    print('Trained Epoch: {} with loss= {:.6f}, accuracy= {:.2f}, and epsilon= {:.2f}'.format(\n",
        "            epoch,\n",
        "            epoch_loss / count,\n",
        "            100. * correct / len(train_loader.dataset),\n",
        "            epsilon\n",
        "    ))\n",
        "    train_losses.append(epoch_loss / count)\n",
        "    # torch.save(model.state_dict(), './saved/model.pth')\n",
        "    # torch.save(optimizer.state_dict(), './saved/optimizer.pth')\n",
        "            \n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, torch.flatten(target)).item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "            count+=1\n",
        "    test_loss /= count\n",
        "    test_losses.append(test_loss)\n",
        "    print('Test set metrics: \\tAvg. loss: {:.4f}, \\tAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)\n",
        "    ))\n",
        "    \n",
        "    \n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "    \n",
        "fig = plt.figure()\n",
        "plt.plot(np.arange(1, 101, 1), train_losses, color='blue')\n",
        "plt.plot(np.arange(1, 101, 1), test_losses, color='red')\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wBCWvIfXQYno",
        "outputId": "56a25e04-0b1c-4669-f349-a8349dbe4b88"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERRORS:  []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/opacus/accountants/analysis/prv/prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
            "  z = np.log((np.exp(t) + q - 1) / q)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using sigma=0.80230712890625 and C=4\n",
            "Training Epoch: 1 [--------------------]\rTraining Epoch: 1 [--------------------] (620/60000)\rTraining Epoch: 1 [--------------------] (1249/60000)\rTraining Epoch: 1 [--------------------] (1848/60000)\rTraining Epoch: 1 [--------------------] (2425/60000)\rTraining Epoch: 1 [=-------------------] (3019/60000)\rTraining Epoch: 1 [=-------------------] (3584/60000)\rTraining Epoch: 1 [=-------------------] (4167/60000)\rTraining Epoch: 1 [=-------------------] (4777/60000)\rTraining Epoch: 1 [=-------------------] (5376/60000)\rTraining Epoch: 1 [=-------------------] (5963/60000)\rTraining Epoch: 1 [==------------------] (6543/60000)\rTraining Epoch: 1 [==------------------] (7175/60000)\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch: 1 [====================] (60018/60000)\n",
            "Trained Epoch: 1 with loss= 1.913042, accuracy= 52.41, and epsilon= 1.51\n",
            "Test set metrics: \tAvg. loss: 1.3592, \tAccuracy: 7938/10000 (79.38%)\n",
            "\n",
            "Training Epoch: 2 [====================] (59664/60000)\n",
            "Trained Epoch: 2 with loss= 0.966873, accuracy= 81.10, and epsilon= 1.80\n",
            "Test set metrics: \tAvg. loss: 0.6643, \tAccuracy: 8634/10000 (86.34%)\n",
            "\n",
            "Training Epoch: 3 [====================] (60276/60000)\n",
            "Trained Epoch: 3 with loss= 0.581773, accuracy= 86.12, and epsilon= 2.02\n",
            "Test set metrics: \tAvg. loss: 0.4766, \tAccuracy: 8782/10000 (87.82%)\n",
            "\n",
            "Training Epoch: 4 [====================] (59640/60000)\n",
            "Trained Epoch: 4 with loss= 0.469457, accuracy= 86.45, and epsilon= 2.21\n",
            "Test set metrics: \tAvg. loss: 0.4148, \tAccuracy: 8848/10000 (88.48%)\n",
            "\n",
            "Training Epoch: 5 [====================] (60017/60000)\n",
            "Trained Epoch: 5 with loss= 0.425297, accuracy= 87.77, and epsilon= 2.39\n",
            "Test set metrics: \tAvg. loss: 0.3848, \tAccuracy: 8903/10000 (89.03%)\n",
            "\n",
            "Training Epoch: 6 [====================] (59646/60000)\n",
            "Trained Epoch: 6 with loss= 0.405082, accuracy= 87.71, and epsilon= 2.55\n",
            "Test set metrics: \tAvg. loss: 0.3696, \tAccuracy: 8920/10000 (89.20%)\n",
            "\n",
            "Training Epoch: 7 [====================] (60207/60000)\n",
            "Trained Epoch: 7 with loss= 0.385958, accuracy= 88.90, and epsilon= 2.71\n",
            "Test set metrics: \tAvg. loss: 0.3599, \tAccuracy: 8934/10000 (89.34%)\n",
            "\n",
            "Training Epoch: 8 [====================] (60221/60000)\n",
            "Trained Epoch: 8 with loss= 0.376220, accuracy= 89.23, and epsilon= 2.85\n",
            "Test set metrics: \tAvg. loss: 0.3563, \tAccuracy: 8943/10000 (89.43%)\n",
            "\n",
            "Training Epoch: 9 [====================] (59848/60000)\n",
            "Trained Epoch: 9 with loss= 0.378202, accuracy= 88.70, and epsilon= 2.99\n",
            "Test set metrics: \tAvg. loss: 0.3519, \tAccuracy: 9002/10000 (90.02%)\n",
            "\n",
            "Training Epoch: 10 [====================] (60112/60000)\n",
            "Trained Epoch: 10 with loss= 0.368593, accuracy= 89.53, and epsilon= 3.13\n",
            "Test set metrics: \tAvg. loss: 0.3434, \tAccuracy: 9016/10000 (90.16%)\n",
            "\n",
            "Training Epoch: 11 [====================] (60722/60000)\n",
            "Trained Epoch: 11 with loss= 0.372941, accuracy= 90.40, and epsilon= 3.26\n",
            "Test set metrics: \tAvg. loss: 0.3349, \tAccuracy: 9060/10000 (90.60%)\n",
            "\n",
            "Training Epoch: 12 [====================] (59954/60000)\n",
            "Trained Epoch: 12 with loss= 0.370089, accuracy= 89.53, and epsilon= 3.38\n",
            "Test set metrics: \tAvg. loss: 0.3360, \tAccuracy: 9062/10000 (90.62%)\n",
            "\n",
            "Training Epoch: 13 [====================] (59857/60000)\n",
            "Trained Epoch: 13 with loss= 0.366884, accuracy= 89.39, and epsilon= 3.50\n",
            "Test set metrics: \tAvg. loss: 0.3372, \tAccuracy: 9079/10000 (90.79%)\n",
            "\n",
            "Training Epoch: 14 [====================] (60071/60000)\n",
            "Trained Epoch: 14 with loss= 0.361379, accuracy= 90.02, and epsilon= 3.62\n",
            "Test set metrics: \tAvg. loss: 0.3365, \tAccuracy: 9079/10000 (90.79%)\n",
            "\n",
            "Training Epoch: 15 [====================] (59724/60000)\n",
            "Trained Epoch: 15 with loss= 0.359927, accuracy= 89.63, and epsilon= 3.74\n",
            "Test set metrics: \tAvg. loss: 0.3393, \tAccuracy: 9069/10000 (90.69%)\n",
            "\n",
            "Training Epoch: 16 [====================] (59843/60000)\n",
            "Trained Epoch: 16 with loss= 0.358285, accuracy= 89.90, and epsilon= 3.85\n",
            "Test set metrics: \tAvg. loss: 0.3396, \tAccuracy: 9091/10000 (90.91%)\n",
            "\n",
            "Training Epoch: 17 [====================] (59823/60000)\n",
            "Trained Epoch: 17 with loss= 0.359367, accuracy= 89.67, and epsilon= 3.96\n",
            "Test set metrics: \tAvg. loss: 0.3415, \tAccuracy: 9085/10000 (90.85%)\n",
            "\n",
            "Training Epoch: 18 [====================] (60160/60000)\n",
            "Trained Epoch: 18 with loss= 0.360547, accuracy= 90.45, and epsilon= 4.06\n",
            "Test set metrics: \tAvg. loss: 0.3391, \tAccuracy: 9088/10000 (90.88%)\n",
            "\n",
            "Training Epoch: 19 [====================] (60318/60000)\n",
            "Trained Epoch: 19 with loss= 0.362532, accuracy= 90.70, and epsilon= 4.17\n",
            "Test set metrics: \tAvg. loss: 0.3425, \tAccuracy: 9083/10000 (90.83%)\n",
            "\n",
            "Training Epoch: 20 [====================] (60027/60000)\n",
            "Trained Epoch: 20 with loss= 0.366761, accuracy= 90.20, and epsilon= 4.27\n",
            "Test set metrics: \tAvg. loss: 0.3383, \tAccuracy: 9118/10000 (91.18%)\n",
            "\n",
            "Training Epoch: 21 [====================] (60328/60000)\n",
            "Trained Epoch: 21 with loss= 0.364517, accuracy= 90.82, and epsilon= 4.37\n",
            "Test set metrics: \tAvg. loss: 0.3427, \tAccuracy: 9104/10000 (91.04%)\n",
            "\n",
            "Training Epoch: 22 [====================] (59879/60000)\n",
            "Trained Epoch: 22 with loss= 0.351277, accuracy= 90.36, and epsilon= 4.47\n",
            "Test set metrics: \tAvg. loss: 0.3391, \tAccuracy: 9113/10000 (91.13%)\n",
            "\n",
            "Training Epoch: 23 [====================] (59820/60000)\n",
            "Trained Epoch: 23 with loss= 0.362298, accuracy= 90.24, and epsilon= 4.57\n",
            "Test set metrics: \tAvg. loss: 0.3399, \tAccuracy: 9123/10000 (91.23%)\n",
            "\n",
            "Training Epoch: 24 [====================] (60004/60000)\n",
            "Trained Epoch: 24 with loss= 0.371191, accuracy= 90.19, and epsilon= 4.67\n",
            "Test set metrics: \tAvg. loss: 0.3411, \tAccuracy: 9123/10000 (91.23%)\n",
            "\n",
            "Training Epoch: 25 [====================] (59534/60000)\n",
            "Trained Epoch: 25 with loss= 0.357189, accuracy= 90.01, and epsilon= 4.76\n",
            "Test set metrics: \tAvg. loss: 0.3386, \tAccuracy: 9127/10000 (91.27%)\n",
            "\n",
            "Training Epoch: 26 [====================] (59595/60000)\n",
            "Trained Epoch: 26 with loss= 0.364254, accuracy= 89.96, and epsilon= 4.85\n",
            "Test set metrics: \tAvg. loss: 0.3440, \tAccuracy: 9127/10000 (91.27%)\n",
            "\n",
            "Training Epoch: 27 [====================] (59922/60000)\n",
            "Trained Epoch: 27 with loss= 0.364348, accuracy= 90.32, and epsilon= 4.95\n",
            "Test set metrics: \tAvg. loss: 0.3430, \tAccuracy: 9121/10000 (91.21%)\n",
            "\n",
            "Training Epoch: 28 [====================] (60002/60000)\n",
            "Trained Epoch: 28 with loss= 0.362402, accuracy= 90.56, and epsilon= 5.04\n",
            "Test set metrics: \tAvg. loss: 0.3404, \tAccuracy: 9116/10000 (91.16%)\n",
            "\n",
            "Training Epoch: 29 [====================] (59842/60000)\n",
            "Trained Epoch: 29 with loss= 0.364814, accuracy= 90.33, and epsilon= 5.13\n",
            "Test set metrics: \tAvg. loss: 0.3377, \tAccuracy: 9128/10000 (91.28%)\n",
            "\n",
            "Training Epoch: 30 [====================] (60281/60000)\n",
            "Trained Epoch: 30 with loss= 0.367444, accuracy= 90.92, and epsilon= 5.21\n",
            "Test set metrics: \tAvg. loss: 0.3433, \tAccuracy: 9118/10000 (91.18%)\n",
            "\n",
            "Training Epoch: 31 [====================] (59790/60000)\n",
            "Trained Epoch: 31 with loss= 0.373568, accuracy= 90.16, and epsilon= 5.30\n",
            "Test set metrics: \tAvg. loss: 0.3457, \tAccuracy: 9105/10000 (91.05%)\n",
            "\n",
            "Training Epoch: 32 [====================] (60499/60000)\n",
            "Trained Epoch: 32 with loss= 0.369963, accuracy= 91.42, and epsilon= 5.39\n",
            "Test set metrics: \tAvg. loss: 0.3369, \tAccuracy: 9142/10000 (91.42%)\n",
            "\n",
            "Training Epoch: 33 [====================] (60166/60000)\n",
            "Trained Epoch: 33 with loss= 0.371310, accuracy= 90.95, and epsilon= 5.47\n",
            "Test set metrics: \tAvg. loss: 0.3443, \tAccuracy: 9142/10000 (91.42%)\n",
            "\n",
            "Training Epoch: 34 [====================] (59985/60000)\n",
            "Trained Epoch: 34 with loss= 0.359154, accuracy= 90.62, and epsilon= 5.56\n",
            "Test set metrics: \tAvg. loss: 0.3340, \tAccuracy: 9159/10000 (91.59%)\n",
            "\n",
            "Training Epoch: 35 [====================] (60085/60000)\n",
            "Trained Epoch: 35 with loss= 0.367295, accuracy= 90.97, and epsilon= 5.64\n",
            "Test set metrics: \tAvg. loss: 0.3405, \tAccuracy: 9151/10000 (91.51%)\n",
            "\n",
            "Training Epoch: 36 [====================] (60051/60000)\n",
            "Trained Epoch: 36 with loss= 0.369762, accuracy= 90.81, and epsilon= 5.72\n",
            "Test set metrics: \tAvg. loss: 0.3445, \tAccuracy: 9152/10000 (91.52%)\n",
            "\n",
            "Training Epoch: 37 [====================] (59788/60000)\n",
            "Trained Epoch: 37 with loss= 0.368310, accuracy= 90.44, and epsilon= 5.81\n",
            "Test set metrics: \tAvg. loss: 0.3355, \tAccuracy: 9162/10000 (91.62%)\n",
            "\n",
            "Training Epoch: 38 [====================] (60035/60000)\n",
            "Trained Epoch: 38 with loss= 0.376448, accuracy= 90.93, and epsilon= 5.89\n",
            "Test set metrics: \tAvg. loss: 0.3380, \tAccuracy: 9156/10000 (91.56%)\n",
            "\n",
            "Training Epoch: 39 [====================] (60033/60000)\n",
            "Trained Epoch: 39 with loss= 0.374242, accuracy= 90.90, and epsilon= 5.97\n",
            "Test set metrics: \tAvg. loss: 0.3405, \tAccuracy: 9167/10000 (91.67%)\n",
            "\n",
            "Training Epoch: 40 [====================] (59776/60000)\n",
            "Trained Epoch: 40 with loss= 0.368604, accuracy= 90.48, and epsilon= 6.05\n",
            "Test set metrics: \tAvg. loss: 0.3437, \tAccuracy: 9159/10000 (91.59%)\n",
            "\n",
            "Training Epoch: 41 [====================] (59973/60000)\n",
            "Trained Epoch: 41 with loss= 0.367306, accuracy= 90.84, and epsilon= 6.12\n",
            "Test set metrics: \tAvg. loss: 0.3406, \tAccuracy: 9162/10000 (91.62%)\n",
            "\n",
            "Training Epoch: 42 [====================] (60117/60000)\n",
            "Trained Epoch: 42 with loss= 0.362256, accuracy= 91.18, and epsilon= 6.20\n",
            "Test set metrics: \tAvg. loss: 0.3456, \tAccuracy: 9167/10000 (91.67%)\n",
            "\n",
            "Training Epoch: 43 [====================] (60250/60000)\n",
            "Trained Epoch: 43 with loss= 0.357775, accuracy= 91.52, and epsilon= 6.28\n",
            "Test set metrics: \tAvg. loss: 0.3507, \tAccuracy: 9152/10000 (91.52%)\n",
            "\n",
            "Training Epoch: 44 [====================] (59914/60000)\n",
            "Trained Epoch: 44 with loss= 0.375021, accuracy= 90.61, and epsilon= 6.36\n",
            "Test set metrics: \tAvg. loss: 0.3481, \tAccuracy: 9131/10000 (91.31%)\n",
            "\n",
            "Training Epoch: 45 [====================] (60018/60000)\n",
            "Trained Epoch: 45 with loss= 0.361787, accuracy= 91.22, and epsilon= 6.43\n",
            "Test set metrics: \tAvg. loss: 0.3461, \tAccuracy: 9136/10000 (91.36%)\n",
            "\n",
            "Training Epoch: 46 [====================] (59996/60000)\n",
            "Trained Epoch: 46 with loss= 0.361761, accuracy= 91.13, and epsilon= 6.51\n",
            "Test set metrics: \tAvg. loss: 0.3415, \tAccuracy: 9151/10000 (91.51%)\n",
            "\n",
            "Training Epoch: 47 [====================] (59926/60000)\n",
            "Trained Epoch: 47 with loss= 0.371934, accuracy= 90.79, and epsilon= 6.58\n",
            "Test set metrics: \tAvg. loss: 0.3406, \tAccuracy: 9150/10000 (91.50%)\n",
            "\n",
            "Training Epoch: 48 [====================] (59957/60000)\n",
            "Trained Epoch: 48 with loss= 0.362175, accuracy= 91.08, and epsilon= 6.66\n",
            "Test set metrics: \tAvg. loss: 0.3409, \tAccuracy: 9163/10000 (91.63%)\n",
            "\n",
            "Training Epoch: 49 [====================] (59974/60000)\n",
            "Trained Epoch: 49 with loss= 0.368653, accuracy= 91.01, and epsilon= 6.73\n",
            "Test set metrics: \tAvg. loss: 0.3476, \tAccuracy: 9166/10000 (91.66%)\n",
            "\n",
            "Training Epoch: 50 [====================] (60379/60000)\n",
            "Trained Epoch: 50 with loss= 0.367843, accuracy= 91.53, and epsilon= 6.80\n",
            "Test set metrics: \tAvg. loss: 0.3515, \tAccuracy: 9159/10000 (91.59%)\n",
            "\n",
            "Training Epoch: 51 [====================] (59837/60000)\n",
            "Trained Epoch: 51 with loss= 0.368942, accuracy= 90.83, and epsilon= 6.88\n",
            "Test set metrics: \tAvg. loss: 0.3432, \tAccuracy: 9174/10000 (91.74%)\n",
            "\n",
            "Training Epoch: 52 [====================] (60082/60000)\n",
            "Trained Epoch: 52 with loss= 0.369859, accuracy= 91.17, and epsilon= 6.95\n",
            "Test set metrics: \tAvg. loss: 0.3453, \tAccuracy: 9184/10000 (91.84%)\n",
            "\n",
            "Training Epoch: 53 [====================] (59667/60000)\n",
            "Trained Epoch: 53 with loss= 0.372613, accuracy= 90.57, and epsilon= 7.02\n",
            "Test set metrics: \tAvg. loss: 0.3557, \tAccuracy: 9165/10000 (91.65%)\n",
            "\n",
            "Training Epoch: 54 [====================] (59671/60000)\n",
            "Trained Epoch: 54 with loss= 0.369126, accuracy= 90.67, and epsilon= 7.09\n",
            "Test set metrics: \tAvg. loss: 0.3436, \tAccuracy: 9196/10000 (91.96%)\n",
            "\n",
            "Training Epoch: 55 [====================] (59953/60000)\n",
            "Trained Epoch: 55 with loss= 0.364861, accuracy= 91.22, and epsilon= 7.16\n",
            "Test set metrics: \tAvg. loss: 0.3487, \tAccuracy: 9170/10000 (91.70%)\n",
            "\n",
            "Training Epoch: 56 [====================] (59867/60000)\n",
            "Trained Epoch: 56 with loss= 0.378263, accuracy= 90.82, and epsilon= 7.23\n",
            "Test set metrics: \tAvg. loss: 0.3475, \tAccuracy: 9181/10000 (91.81%)\n",
            "\n",
            "Training Epoch: 57 [====================] (59926/60000)\n",
            "Trained Epoch: 57 with loss= 0.364984, accuracy= 91.04, and epsilon= 7.30\n",
            "Test set metrics: \tAvg. loss: 0.3404, \tAccuracy: 9204/10000 (92.04%)\n",
            "\n",
            "Training Epoch: 58 [====================] (60307/60000)\n",
            "Trained Epoch: 58 with loss= 0.367198, accuracy= 91.74, and epsilon= 7.37\n",
            "Test set metrics: \tAvg. loss: 0.3436, \tAccuracy: 9192/10000 (91.92%)\n",
            "\n",
            "Training Epoch: 59 [====================] (59777/60000)\n",
            "Trained Epoch: 59 with loss= 0.369239, accuracy= 90.86, and epsilon= 7.44\n",
            "Test set metrics: \tAvg. loss: 0.3450, \tAccuracy: 9202/10000 (92.02%)\n",
            "\n",
            "Training Epoch: 60 [====================] (59435/60000)\n",
            "Trained Epoch: 60 with loss= 0.377469, accuracy= 90.34, and epsilon= 7.51\n",
            "Test set metrics: \tAvg. loss: 0.3442, \tAccuracy: 9190/10000 (91.90%)\n",
            "\n",
            "Training Epoch: 61 [====================] (60046/60000)\n",
            "Trained Epoch: 61 with loss= 0.363399, accuracy= 91.45, and epsilon= 7.58\n",
            "Test set metrics: \tAvg. loss: 0.3462, \tAccuracy: 9182/10000 (91.82%)\n",
            "\n",
            "Training Epoch: 62 [====================] (60122/60000)\n",
            "Trained Epoch: 62 with loss= 0.369193, accuracy= 91.45, and epsilon= 7.64\n",
            "Test set metrics: \tAvg. loss: 0.3498, \tAccuracy: 9192/10000 (91.92%)\n",
            "\n",
            "Training Epoch: 63 [====================] (59936/60000)\n",
            "Trained Epoch: 63 with loss= 0.368250, accuracy= 91.14, and epsilon= 7.71\n",
            "Test set metrics: \tAvg. loss: 0.3438, \tAccuracy: 9200/10000 (92.00%)\n",
            "\n",
            "Training Epoch: 64 [====================] (60576/60000)\n",
            "Trained Epoch: 64 with loss= 0.362760, accuracy= 92.24, and epsilon= 7.78\n",
            "Test set metrics: \tAvg. loss: 0.3442, \tAccuracy: 9183/10000 (91.83%)\n",
            "\n",
            "Training Epoch: 65 [====================] (59900/60000)\n",
            "Trained Epoch: 65 with loss= 0.369835, accuracy= 91.03, and epsilon= 7.85\n",
            "Test set metrics: \tAvg. loss: 0.3460, \tAccuracy: 9202/10000 (92.02%)\n",
            "\n",
            "Training Epoch: 66 [====================] (60062/60000)\n",
            "Trained Epoch: 66 with loss= 0.363842, accuracy= 91.44, and epsilon= 7.91\n",
            "Test set metrics: \tAvg. loss: 0.3410, \tAccuracy: 9187/10000 (91.87%)\n",
            "\n",
            "Training Epoch: 67 [====================] (59944/60000)\n",
            "Trained Epoch: 67 with loss= 0.358379, accuracy= 91.35, and epsilon= 7.98\n",
            "Test set metrics: \tAvg. loss: 0.3423, \tAccuracy: 9180/10000 (91.80%)\n",
            "\n",
            "Training Epoch: 68 [====================] (59653/60000)\n",
            "Trained Epoch: 68 with loss= 0.367385, accuracy= 90.77, and epsilon= 8.04\n",
            "Test set metrics: \tAvg. loss: 0.3447, \tAccuracy: 9190/10000 (91.90%)\n",
            "\n",
            "Training Epoch: 69 [====================] (59711/60000)\n",
            "Trained Epoch: 69 with loss= 0.370836, accuracy= 90.81, and epsilon= 8.11\n",
            "Test set metrics: \tAvg. loss: 0.3401, \tAccuracy: 9198/10000 (91.98%)\n",
            "\n",
            "Training Epoch: 70 [====================] (59851/60000)\n",
            "Trained Epoch: 70 with loss= 0.368531, accuracy= 91.03, and epsilon= 8.17\n",
            "Test set metrics: \tAvg. loss: 0.3460, \tAccuracy: 9188/10000 (91.88%)\n",
            "\n",
            "Training Epoch: 71 [====================] (60004/60000)\n",
            "Trained Epoch: 71 with loss= 0.371654, accuracy= 91.21, and epsilon= 8.24\n",
            "Test set metrics: \tAvg. loss: 0.3442, \tAccuracy: 9189/10000 (91.89%)\n",
            "\n",
            "Training Epoch: 72 [====================] (59484/60000)\n",
            "Trained Epoch: 72 with loss= 0.358257, accuracy= 90.90, and epsilon= 8.30\n",
            "Test set metrics: \tAvg. loss: 0.3442, \tAccuracy: 9165/10000 (91.65%)\n",
            "\n",
            "Training Epoch: 73 [====================] (60461/60000)\n",
            "Trained Epoch: 73 with loss= 0.367102, accuracy= 92.04, and epsilon= 8.37\n",
            "Test set metrics: \tAvg. loss: 0.3423, \tAccuracy: 9176/10000 (91.76%)\n",
            "\n",
            "Training Epoch: 74 [====================] (59951/60000)\n",
            "Trained Epoch: 74 with loss= 0.364900, accuracy= 91.45, and epsilon= 8.43\n",
            "Test set metrics: \tAvg. loss: 0.3434, \tAccuracy: 9201/10000 (92.01%)\n",
            "\n",
            "Training Epoch: 75 [====================] (60485/60000)\n",
            "Trained Epoch: 75 with loss= 0.363256, accuracy= 92.07, and epsilon= 8.49\n",
            "Test set metrics: \tAvg. loss: 0.3470, \tAccuracy: 9200/10000 (92.00%)\n",
            "\n",
            "Training Epoch: 76 [====================] (60304/60000)\n",
            "Trained Epoch: 76 with loss= 0.371532, accuracy= 91.85, and epsilon= 8.56\n",
            "Test set metrics: \tAvg. loss: 0.3496, \tAccuracy: 9198/10000 (91.98%)\n",
            "\n",
            "Training Epoch: 77 [====================] (59871/60000)\n",
            "Trained Epoch: 77 with loss= 0.365169, accuracy= 91.32, and epsilon= 8.62\n",
            "Test set metrics: \tAvg. loss: 0.3471, \tAccuracy: 9209/10000 (92.09%)\n",
            "\n",
            "Training Epoch: 78 [====================] (60043/60000)\n",
            "Trained Epoch: 78 with loss= 0.356543, accuracy= 91.76, and epsilon= 8.68\n",
            "Test set metrics: \tAvg. loss: 0.3471, \tAccuracy: 9197/10000 (91.97%)\n",
            "\n",
            "Training Epoch: 79 [====================] (59702/60000)\n",
            "Trained Epoch: 79 with loss= 0.369184, accuracy= 91.18, and epsilon= 8.74\n",
            "Test set metrics: \tAvg. loss: 0.3469, \tAccuracy: 9217/10000 (92.17%)\n",
            "\n",
            "Training Epoch: 80 [====================] (60128/60000)\n",
            "Trained Epoch: 80 with loss= 0.351668, accuracy= 91.94, and epsilon= 8.81\n",
            "Test set metrics: \tAvg. loss: 0.3532, \tAccuracy: 9196/10000 (91.96%)\n",
            "\n",
            "Training Epoch: 81 [====================] (60063/60000)\n",
            "Trained Epoch: 81 with loss= 0.364525, accuracy= 91.61, and epsilon= 8.87\n",
            "Test set metrics: \tAvg. loss: 0.3514, \tAccuracy: 9200/10000 (92.00%)\n",
            "\n",
            "Training Epoch: 82 [====================] (60271/60000)\n",
            "Trained Epoch: 82 with loss= 0.361503, accuracy= 92.04, and epsilon= 8.93\n",
            "Test set metrics: \tAvg. loss: 0.3449, \tAccuracy: 9229/10000 (92.29%)\n",
            "\n",
            "Training Epoch: 83 [====================] (60141/60000)\n",
            "Trained Epoch: 83 with loss= 0.357274, accuracy= 91.84, and epsilon= 8.99\n",
            "Test set metrics: \tAvg. loss: 0.3500, \tAccuracy: 9233/10000 (92.33%)\n",
            "\n",
            "Training Epoch: 84 [====================] (60050/60000)\n",
            "Trained Epoch: 84 with loss= 0.380920, accuracy= 91.54, and epsilon= 9.05\n",
            "Test set metrics: \tAvg. loss: 0.3480, \tAccuracy: 9205/10000 (92.05%)\n",
            "\n",
            "Training Epoch: 85 [====================] (59639/60000)\n",
            "Trained Epoch: 85 with loss= 0.372043, accuracy= 91.00, and epsilon= 9.11\n",
            "Test set metrics: \tAvg. loss: 0.3440, \tAccuracy: 9226/10000 (92.26%)\n",
            "\n",
            "Training Epoch: 86 [====================] (59897/60000)\n",
            "Trained Epoch: 86 with loss= 0.360742, accuracy= 91.42, and epsilon= 9.17\n",
            "Test set metrics: \tAvg. loss: 0.3425, \tAccuracy: 9229/10000 (92.29%)\n",
            "\n",
            "Training Epoch: 87 [====================] (60266/60000)\n",
            "Trained Epoch: 87 with loss= 0.362048, accuracy= 92.05, and epsilon= 9.23\n",
            "Test set metrics: \tAvg. loss: 0.3497, \tAccuracy: 9223/10000 (92.23%)\n",
            "\n",
            "Training Epoch: 88 [====================] (59703/60000)\n",
            "Trained Epoch: 88 with loss= 0.365508, accuracy= 91.02, and epsilon= 9.29\n",
            "Test set metrics: \tAvg. loss: 0.3494, \tAccuracy: 9196/10000 (91.96%)\n",
            "\n",
            "Training Epoch: 89 [====================] (59596/60000)\n",
            "Trained Epoch: 89 with loss= 0.367222, accuracy= 90.99, and epsilon= 9.35\n",
            "Test set metrics: \tAvg. loss: 0.3502, \tAccuracy: 9219/10000 (92.19%)\n",
            "\n",
            "Training Epoch: 90 [====================] (59959/60000)\n",
            "Trained Epoch: 90 with loss= 0.363085, accuracy= 91.61, and epsilon= 9.41\n",
            "Test set metrics: \tAvg. loss: 0.3495, \tAccuracy: 9218/10000 (92.18%)\n",
            "\n",
            "Training Epoch: 91 [====================] (60184/60000)\n",
            "Trained Epoch: 91 with loss= 0.359579, accuracy= 91.96, and epsilon= 9.47\n",
            "Test set metrics: \tAvg. loss: 0.3438, \tAccuracy: 9218/10000 (92.18%)\n",
            "\n",
            "Training Epoch: 92 [====================] (59810/60000)\n",
            "Trained Epoch: 92 with loss= 0.367651, accuracy= 91.50, and epsilon= 9.53\n",
            "Test set metrics: \tAvg. loss: 0.3456, \tAccuracy: 9198/10000 (91.98%)\n",
            "\n",
            "Training Epoch: 93 [====================] (60217/60000)\n",
            "Trained Epoch: 93 with loss= 0.357583, accuracy= 92.13, and epsilon= 9.59\n",
            "Test set metrics: \tAvg. loss: 0.3454, \tAccuracy: 9226/10000 (92.26%)\n",
            "\n",
            "Training Epoch: 94 [====================] (59872/60000)\n",
            "Trained Epoch: 94 with loss= 0.361487, accuracy= 91.48, and epsilon= 9.65\n",
            "Test set metrics: \tAvg. loss: 0.3465, \tAccuracy: 9226/10000 (92.26%)\n",
            "\n",
            "Training Epoch: 95 [====================] (60106/60000)\n",
            "Trained Epoch: 95 with loss= 0.368167, accuracy= 91.83, and epsilon= 9.70\n",
            "Test set metrics: \tAvg. loss: 0.3515, \tAccuracy: 9216/10000 (92.16%)\n",
            "\n",
            "Training Epoch: 96 [====================] (59826/60000)\n",
            "Trained Epoch: 96 with loss= 0.368895, accuracy= 91.34, and epsilon= 9.76\n",
            "Test set metrics: \tAvg. loss: 0.3423, \tAccuracy: 9232/10000 (92.32%)\n",
            "\n",
            "Training Epoch: 97 [====================] (59833/60000)\n",
            "Trained Epoch: 97 with loss= 0.360612, accuracy= 91.61, and epsilon= 9.82\n",
            "Test set metrics: \tAvg. loss: 0.3465, \tAccuracy: 9225/10000 (92.25%)\n",
            "\n",
            "Training Epoch: 98 [====================] (60130/60000)\n",
            "Trained Epoch: 98 with loss= 0.358007, accuracy= 92.08, and epsilon= 9.88\n",
            "Test set metrics: \tAvg. loss: 0.3482, \tAccuracy: 9210/10000 (92.10%)\n",
            "\n",
            "Training Epoch: 99 [====================] (60401/60000)\n",
            "Trained Epoch: 99 with loss= 0.355895, accuracy= 92.49, and epsilon= 9.94\n",
            "Test set metrics: \tAvg. loss: 0.3410, \tAccuracy: 9221/10000 (92.21%)\n",
            "\n",
            "Training Epoch: 100 [====================] (59683/60000)\n",
            "Trained Epoch: 100 with loss= 0.359052, accuracy= 91.27, and epsilon= 9.99\n",
            "Test set metrics: \tAvg. loss: 0.3448, \tAccuracy: 9212/10000 (92.12%)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O783mrK8QYrB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}